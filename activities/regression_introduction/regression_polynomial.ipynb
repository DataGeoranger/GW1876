{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regression using a Polynomial and  least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import scipy.optimize as spo\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up a vector of x values using polynomial parameters\n",
    "best_degree = 2\n",
    "np.random.seed(2)\n",
    "polypars = np.random.normal(loc=1, scale=3,size=best_degree+1)\n",
    "polypars[-1]=0\n",
    "print(polypars)\n",
    "# set number of polynomial degrees to evaluate\n",
    "total_degrees = 10\n",
    "\n",
    "# set up the degree range for plotting\n",
    "degree_range = [i+1 for i in range(total_degrees)]\n",
    "\n",
    "offset = 0.25\n",
    "minx = -3\n",
    "maxx = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First cook up some data\n",
    "\n",
    "Set a range of x-values, then make a \"true\" set of y-values using a second degree polynomial (e.g. `best_degree=2`).\n",
    "\n",
    "The general form of the polynomial is: \n",
    "## $y=ax^2 + bx + c$\n",
    "\n",
    "We can set $c=0$ for our purposes so we have two free parameters: $a$ and $b$\n",
    "\n",
    "Also add noise to the \"true\" observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(minx,maxx,100)\n",
    "xplot = np.linspace(x[0]-offset,x[-1]+offset,1000)\n",
    "\n",
    "poly_func = np.poly1d(polypars)\n",
    "y = poly_func(x)\n",
    "# set up a y_data vector that has been corrupted by a little noise\n",
    "y_data = y + np.random.normal(loc=0,scale=np.abs(x*(np.abs(y[-1]-y[0]))*0.25), size=len(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to define a couple functions\n",
    "\n",
    "The `parabola` function calculates the equation for a parabola. Note that `c` is skipped since we set it to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parabola(a,b,x_vec):\n",
    "    y = [a*x**2 + b*x for x in x_vec]\n",
    "    return np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `errfun` function calculates a vector of the differences between a vector of data (`y`) and the parabola function estimates at the same locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def errfun(pars,x,y):\n",
    "    return y-parabola(*pars,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data and the true model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(xplot, poly_func(xplot), '-.', lw=2)\n",
    "plt.plot(x,y_data,'o')\n",
    "plt.legend(('True y', 'Noisy y'), loc='best')\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a function to calculate the sum of squared errors\n",
    "\n",
    "This function calculates the sum of squared errors between observations and modeled equivalents. It is more general than `errfun` above which only works with the parabola function:\n",
    "\n",
    "## $SSE=\\Phi=\\sum_{i=1}^{NPAR}\\left(y_{i}-m\\left(x_{i}\\right)\\right)^{2}$\n",
    "where: \n",
    " ## * $SSE$ is sum of squared errors\n",
    " ## * $y_i$ is the $i^{th}$ observation\n",
    " ## * $m\\left(x_i\\right)$ is the modeled equivalent to the $i^{th}$ observation\n",
    " \n",
    "In vector notation, this is expressed as:\n",
    "## $\\Phi=\\left(\\mathbf{y}-\\mathbf{m}\\right)^{T}\\left(\\mathbf{y}-\\mathbf{m}\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_squared_errors(y,m):\n",
    "    y = np.atleast_1d(y)\n",
    "    m = np.atleast_1d(m)\n",
    "    sse = np.dot((y-m).T,(y-m))\n",
    "    return sse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use $\\Phi$ to evaluate the response surface\n",
    "\n",
    "We can look at the response to change is parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.linspace(polypars[0]-1.5,polypars[0]+1.5,20)\n",
    "b = np.linspace(polypars[1]-5,polypars[1]+5,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sse_a = []\n",
    "for i in a:\n",
    "    cfunc = np.poly1d([i,polypars[1],1])\n",
    "    c_calculated = cfunc(x)\n",
    "    sse_a.append(sum_squared_errors(y_data,c_calculated))\n",
    "sse_b = []\n",
    "for i in b:\n",
    "    cfunc = np.poly1d([polypars[0],i,1])\n",
    "    c_calculated = cfunc(x)\n",
    "    sse_b.append(sum_squared_errors(y_data,c_calculated))\n",
    "fig,ax=fig, (ax0, ax1) = plt.subplots(nrows=2)\n",
    "ax0.plot(a,sse_a,'.-')\n",
    "ax0.set_title('SSE as a function of a')\n",
    "ax1.plot(b,sse_b,'.-')\n",
    "ax1.set_title('SSE as a function of b')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A,B = np.meshgrid(a,b)\n",
    "SSE_AB = np.zeros_like(A)\n",
    "for i,junk in np.ndenumerate(SSE_AB):\n",
    "    cfunc=np.poly1d([A[i], B[i], 0])\n",
    "    SSE_AB[i] = sum_squared_errors(y_data,cfunc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "p = ax.pcolor(A,B, SSE_AB,cmap=\"spectral\",alpha=0.5)\n",
    "plt.colorbar(p)\n",
    "\n",
    "c = ax.contour(A,B, SSE_AB,levels=[500,1000,2500,6000],colors='k')\n",
    "plt.clabel(c)\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(A,B,SSE_AB, rstride=2, cstride=2, alpha=0.3)\n",
    "cset = ax.contourf(A,B,SSE_AB, zdir='x',  offset=np.min(a),cmap='spectral')\n",
    "cset = ax.contourf(A,B,SSE_AB, zdir='y',  offset=np.max(b),cmap='spectral')\n",
    "cset = ax.contourf(A,B,SSE_AB, zdir='z', offset=np.min(SSE_AB), cmap='spectral')\n",
    "ax.set_xlabel('a')\n",
    "ax.set_xlim(a[0],a[-1])\n",
    "ax.set_ylabel('b')\n",
    "ax.set_ylim(b[0],b[-1])\n",
    "ax.set_zlabel('SSE')\n",
    "ax.set_zlim(np.min(SSE_AB),np.max(SSE_AB))\n",
    "plt.tight_layout()\n",
    "import matplotlib.cm as cm\n",
    "m = cm.ScalarMappable(cmap='spectral')\n",
    "m.set_array(cset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a polynomial function\n",
    "\n",
    "Now fit a function assuming it will be a polynomial of the same degree (e.g. `best_degree`) as was used to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the polynomial and then plot the resulting function\n",
    "sol = spo.least_squares(errfun,[-2,2],args=(x,y_data))\n",
    "y_fit_pars_best = [*sol.x,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the Jacobian matrix---gradients of parameters wrt. observations\n",
    "\n",
    "For each parameter-observation combination, we can see how much the observation value changes due to a small change in the parameter. If $y$ are the observations and $x$ are the parameters, the equation for the $i^th$ observation with respect to the $j^th$ parameter is:  \n",
    "## $\\frac{\\partial y_i}{\\partial x_j}$\n",
    "This can be approximated by finite differences as :  \n",
    "## $\\frac{\\partial y_i}{\\partial x_j}~\\frac{y\\left(x+\\Delta x \\right)-y\\left(x\\right)}{\\Delta x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,ax=fig, (ax0, ax1) = plt.subplots(nrows=2)\n",
    "ax0.plot(sol.jac[:,0],'.-')\n",
    "ax0.grid('on')\n",
    "ax0.set_title('Jacobian for parameter a')\n",
    "ax1.plot(sol.jac[:,1],'.-')\n",
    "plt.grid('on')\n",
    "ax1.set_title('Jacobian for parameter b')\n",
    "ax1.set_xlabel('Observation Number')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func_fit_best = np.poly1d(y_fit_pars_best)\n",
    "plt.figure(figsize=(5,5))\n",
    "xfitlox = np.linspace(x[0]-offset,x[-1]+offset,100)\n",
    "plt.plot(xfitlox, poly_func(xfitlox), '-.', lw=2)\n",
    "plt.plot(x,y_data,'o')\n",
    "plt.grid('on')\n",
    "plt.plot(xfitlox,func_fit_best(xfitlox), 'r-', lw=2)\n",
    "plt.legend(('True y', 'Noisy y', 'y_fit'), loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('True parameters are:              a={0:.4f}, b={1:.4f}, c={2}'.format(*polypars))\n",
    "print('The best-estimate parameters are: a={0:.4f}, b={1:.4f}, c={2}'.format(*y_fit_pars_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set the x prediction beyond the end of the x range\n",
    "range_x = (x[-1]-x[0])\n",
    "x_pred = x[-1]+range_x*0.21\n",
    "y_pred = poly_func(x_pred)\n",
    "x_predlocations = np.linspace(x[0]-range_x*0.2, x_pred, 1000)\n",
    "# plot the prediction\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(xfitlox, poly_func(xfitlox), '-.',lw=2)\n",
    "plt.plot(x,y_data,'o')\n",
    "plt.plot(xfitlox,func_fit_best(xfitlox), 'r-',lw=2)\n",
    "\n",
    "datafit = sum_squared_errors(y_data,func_fit_best(x))\n",
    "predfit = sum_squared_errors(func_fit_best(x_pred),poly_func(x_pred))\n",
    "\n",
    "plt.plot(x_pred,y_pred, 'o', markerfacecolor='w')\n",
    "plt.plot(x_pred,func_fit_best(x_pred), 'x')\n",
    "plt.title('SSE data = {0:.3f} SSE pred = {1:.3f}'.format(datafit,predfit))\n",
    "plt.legend(('True y', 'Noisy y', 'y_fit', 'y_pred_true', 'y_pred_fit'), loc='best')\n",
    "plt.grid('on')\n",
    "\n",
    "for i in range(best_degree):\n",
    "    y_fit_pars = np.polyfit(x,y_data,i+1)\n",
    "    func_fit = np.poly1d(y_fit_pars)\n",
    "    plt.plot(xfitlox, func_fit(xfitlox), 'k-', alpha=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what happens over a range of polynomial values\n",
    "We can change the `offset` parameter to plot further outside the data range\n",
    "\n",
    "We can also change the `extra_degrees` variable to plot curves of higher polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the curves\n",
    "offset=0\n",
    "extra_degrees = 3\n",
    "xplot_fine = np.linspace(x[0]-offset,x[-1]+offset,1000)\n",
    "\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = fig.add_subplot(111)\n",
    "func_fit = np.poly1d(y_fit_pars_best)\n",
    "plt.plot(x_predlocations, poly_func(x_predlocations), '-.',lw=2)\n",
    "plt.plot(x,y_data,'o')\n",
    "plt.plot(x_predlocations,func_fit(x_predlocations), 'r-', linewidth=2)\n",
    "\n",
    "plt.legend(('True y', 'Noisy y', 'y_fit'), loc='best')\n",
    "all_datafit = list()\n",
    "all_predfit = list()\n",
    "for cdegree in degree_range:\n",
    "    y_fit_pars = np.polyfit(x,y_data,cdegree)\n",
    "    func_fit = np.poly1d(y_fit_pars)\n",
    "    all_datafit.append(sum_squared_errors(y_data,func_fit(x)))\n",
    "    all_predfit.append(sum_squared_errors(func_fit(x_pred),poly_func(x_pred)))\n",
    "    plt.plot(xplot_fine, func_fit(xplot_fine), 'k-', alpha=0.2)\n",
    "    if cdegree<best_degree+extra_degrees:\n",
    "        plt.plot(x_pred,func_fit(x_pred), 'kx', alpha=0.5)\n",
    "        plt.plot(x_predlocations, func_fit(x_predlocations), 'k-', alpha=0.2)\n",
    "\n",
    "    if cdegree == best_degree:\n",
    "        plt.plot(x_pred,func_fit(x_pred), 'rx')\n",
    "\n",
    "plt.plot(x_pred,poly_func(x_pred), 'o', markerfacecolor='w')\n",
    "plt.grid('on')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "all_datafit = np.array(all_datafit)\n",
    "all_predfit = np.array(all_predfit)\n",
    "plt.plot(degree_range,all_datafit, 'bo-')\n",
    "plt.plot(degree_range,all_predfit, 'ro-')\n",
    "plt.plot(degree_range, all_predfit + all_datafit, 'k--')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Polynomial Function Degree')\n",
    "plt.ylabel('Error (SSE)')\n",
    "plt.legend(('Data Error', 'Prediction Error', 'Total Error'), loc='best')\n",
    "plt.title('Error Tradeoff: True data degree = {0}'.format(best_degree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig,(ax0,ax1) = plt.subplots(nrows=2,figsize=(8,5))\n",
    "ax0.set_title('Error Tradeoff: True data degree = {0}'.format(best_degree))\n",
    "ax0.plot(degree_range,all_predfit, 'ro-')\n",
    "ax0.plot(degree_range, all_predfit + all_datafit, 'k--')\n",
    "ax0.set_yscale('log')\n",
    "ax0.set_ylabel('Error (SSE)')\n",
    "ax0.legend(('Prediction Error', 'Total Error'), loc='best')\n",
    "ax1.plot(degree_range,all_datafit, 'bo-')\n",
    "\n",
    "plt.xlabel('Polynomial Function Degree')\n",
    "plt.ylabel('Error (SSE)')\n",
    "plt.legend(['Data Error'], loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
