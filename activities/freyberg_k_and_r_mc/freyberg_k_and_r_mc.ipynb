{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PEST++V3_cover.jpeg\" style=\"float: left\">\n",
    "\n",
    "<img src=\"flopylogo.png\" style=\"float: right\">\n",
    "\n",
    "<img src=\"AW&H2015.png\" style=\"float: center\">\n",
    "\n",
    "# Linear/FOSM uncertainty is cool, but for the NEXT BIG THING, you gotta go Monte Carlo \n",
    "\n",
    "## (what's old is new again)\n",
    "\n",
    "As we've seen, First-Order-Second-Moment (FOSM) is quick and insightful.  But FOSM depends on an assumption that the relation between the model and the forecast uncertainty is linear.  But many times the world is nonlinear. Short cuts like FOSM need assumptions, but we can free ourselves by taking the brute force approach.  That is define the parameters that are important, provide the prior uncertainty, sample those parameters many times, run the model many times, and then summarize the results.  \n",
    "\n",
    "On a more practical standpoint, the underlying theory for FOSM and why it results in shortcomings can be hard to explain to stakeholders.  Monte Carlo, however, is VERY straightforward, its computational brute force notwithstanding. \n",
    "\n",
    "### Here's a flowchart from Anderson et al. (2015):\n",
    "\n",
    "<img src=\"Fig10.14_MC_workflow.png\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we get when we do this?  Here are results from a MODFLOW-Monte Carlo model that was built to determine the capture zone for a spring (Hunt et al., 2001). They showe that we can get average heads for all the runs:\n",
    "\n",
    "<img src=\"PB_avg_heads_Hunt2001.png\" style=\"float: center\">\n",
    "\n",
    "But even cooler - we can get a map of standard deviation of those heads:\n",
    "\n",
    "<img src=\"PB_stdev_heads_Hunt2001.png\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even cooler - the forecast for the area that provdes recharge to the spring can be related to stakeholders probabilistically!\n",
    "\n",
    "<img src=\"FigB10.4.2_MC_probablistic_capture_zone.png\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives of this notebook\n",
    "\n",
    "1) Run a real Monte Carlo run on the Freyberg model\n",
    "\n",
    "2) Look at parameter and forecast uncertainty \n",
    "\n",
    "3) Start thinking of the advantages and disadvantages of linear and nonlinear uncertainty methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard two blocks needed to prep the notebook for what we cant to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyemu\n",
    "import zipfile\n",
    "shutil.copy2(os.path.join('..','freyberg_k_and_r_response_surface','sweep.zip'),\n",
    "             os.path.join(working_dir,'sweep.zip'))\n",
    "with zipfile.ZipFile(os.path.join(working_dir,'sweep.zip')) as inzip:\n",
    "    inzip.extractall(working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import freyberg_setup as fs\n",
    "fs.setup_pest_kr(make_porosity_tpl=True)\n",
    "working_dir = fs.WORKING_DIR_KR\n",
    "pst_name = fs.PST_NAME_KR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the K field (left) and simulated heads and travel time (right)\n",
    "fs.plot_model(working_dir, pst_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some Monte Carlo\n",
    "\n",
    "Monte Carlo uses a lots and lots of forward runs so we don't want to make the mistake of burning the silicon for a PEST control file that is not right.  Here we make doubly sure that the control file has the recharge freed (not \"fixed' in the PEST control file).  Note you can use this block in the future for your models...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "obs = pst.observation_data\n",
    "obs.loc[obs.obgnme==\"calflux\",\"weight\"] = 0.05#super subjective\n",
    "pst.write(os.path.join(working_dir,pst_name))\n",
    "pst.parameter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah - the recharge for the first stress period (rch_0) is fixed - that is no good. But, really, by leaving the recharge for the future (rch_1) fixed we are also saying that we know it perfectly when we know it even less than the the calibration period rch_0.  Being the purpose of the model is to forecast the future, in our Monte Carlo we may want to include this.  Let's use pyemu to reset both rch_0 and rch_1 (though a text editor is fine too). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.loc[[\"rch_0\",\"rch_1\"],\"partrans\"] = \"log\"\n",
    "pst.write(os.path.join(working_dir,pst_name))\n",
    "pst.parameter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good - past and future recharge are freed.  Now let's draw 500 tries from the parameter set.  \n",
    "\n",
    "Look in the command below - how does it decide the range of parameters to pull from....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_cov = pyemu.Cov.from_parameter_data(pst,sigma_range=6)\n",
    "parensemble = pyemu.ParameterEnsemble.from_gaussian_draw(pst=pst,cov=prior_cov,num_reals=500)\n",
    "parensemble.enforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Here's some parameter distributions you could use to control the values pulled from the parameter range (taken from Anderson et al. 2015).  In the code block you just ran we're using a Gaussian, or \"Normal\" distribution.\n",
    "\n",
    "<img src=\"Fig10.13_parameter_distribution.png\" style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Here's an example of the first 5 parameter sets of our 500 created by our draw (\"draw\" here is like \"drawing\" a card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distributions we are running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pname in pst.par_names:\n",
    "    ax = parensemble.loc[:,pname].hist(bins=20)\n",
    "    print(parensemble.loc[:,pname].min(),parensemble.loc[:,pname].max(),parensemble.loc[:,pname].mean())\n",
    "    ax.set_title(pname)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare these distributions to the upper and lower bounds of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# echo out the parameter data \n",
    "pst.parameter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the histograms above, do the log transformed parameters look like a log normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An aside on covariance and how it ripples to the parameter draws..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficient = -0.8\n",
    "pst1 = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "par = pst1.parameter_data\n",
    "par.loc[:,\"partrans\"] = 'fixed'\n",
    "par.loc[[\"hk\",\"rch_0\"],\"partrans\"] = \"none\"\n",
    "par.loc[\"hk\",\"parval1\"] = 30\n",
    "prior_cov = pyemu.Cov.from_parameter_data(pst1,sigma_range=8)\n",
    "prior_cov = pyemu.Cov(prior_cov.as_2d,names=prior_cov.row_names)\n",
    "c = float(prior_cov.x[0,0] * prior_cov.x[1,1] * correlation_coefficient)\n",
    "#print(c)\n",
    "prior_cov.x[0,1] = c\n",
    "prior_cov.x[1,0] = c\n",
    "#print(prior_cov)\n",
    "#mc.draw(num_reals=2000,enforce_bounds=\"reset\")\n",
    "pe = pyemu.ParameterEnsemble.from_gaussian_draw(pst=pst1,cov=prior_cov,\n",
    "                                                         use_homegrown=True,\n",
    "                                                         num_reals=2000,\n",
    "                                                         group_chunks=False)\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.scatter(pe.hk,pe.rch_0)\n",
    "ax.set_xlim(par.loc[\"hk\",\"parlbnd\"],par.loc[\"hk\",\"parubnd\"])\n",
    "ax.set_ylim(par.loc[\"rch_0\",\"parlbnd\"],par.loc[\"rch_0\",\"parubnd\"])\n",
    "ax.set_xlabel(\"hk\")\n",
    "ax.set_ylabel(\"rch_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the value of correlation_coefficient at the top of the code block above. Note what happens when you get too large, and what happens as you get near zero...how would this ripple to which runs we keep in our Monte Carlo run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enough digression....\n",
    "\n",
    "Using what you just learned, now we'll make sweep through the possible combinations and distribute them across the multicores available on your laptop (just like in the response surface notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  We are going to run the forward model 500 times (!!!!) so it will take some time. \n",
    "\n",
    "using ``sweep`` - like we just talked about, watch the C= part of the YAMR progress line - this is the runs __C__ompleted. F = __F__ailed; T = __T__imed out; R = __R__unning; W = __W__aiting to run; U = __U__navailable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parensemble.to_csv(os.path.join(working_dir,\"sweep_in.csv\"))\n",
    "os.chdir(working_dir)\n",
    "pyemu.os_utils.start_slaves('.',\"pestpp-swp\",pst_name,num_slaves=15,master_dir='.')\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alright - let's see some MC results.  For these runs, what was the Phi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.read_csv(os.path.join(working_dir,\"sweep_out.csv\"),index_col=0)\n",
    "df_out = df_out.loc[df_out.failed_flag==0,:] #drop an failed runs\n",
    "df_out.columns = [c.lower() for c in df_out.columns]\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow, some are pretty large.  Let's plot Phi for all 500 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.phi.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow, some of those models are really REALLY bad fits to the observations.  So when we only use our prior knowledge to sample the parameters we get a bunch of models with unacceptably Phi, and we can consider them not reasonable.  Therefore, we should NOT include them in our uncertainty analysis.\n",
    "\n",
    "\n",
    "# PAY ATTENTION:  this is super important - in this next block we are \"conditioning\" our Monte Carlo run by removing the bad runs based on a Phi cutoff. So here is where we choose which realizations we consider __good enough__ with respect to fitting the observation data. \n",
    "\n",
    "# Those realizations with a phi bigger than our cutoff are out, no matter how close to the cutoff; those that are within the cutoff are all considered equally good regardless of how close to the cutoff they reside.\n",
    "\n",
    "(Jeremy can go all pointy headed on this as this what is also known as \"GLUE\" in the literature.)  \n",
    "\n",
    "## Let's say we'll say any Phi below 1500 is acceptable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_phi = 1500.0\n",
    "good_enough = df_out.loc[df_out.phi<acceptable_phi].index.values\n",
    "print(good_enough.shape,good_enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the run number of the 500 that met this cutoff.  Let's plot just these \"good\" ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_out.phi.hist(alpha=0.5)\n",
    "df_out.loc[good_enough,\"phi\"].hist(color=\"g\",alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import response_surface as rs\n",
    "fig,ax = rs.plot_response_surface(parnames=[\"hk\",\"rch_0\"],\n",
    "                                  pstfile=pst_name.replace(\".pst\",\".r3.pst\"),\n",
    "                                  WORKING_DIR=working_dir,alpha=0.35,\n",
    "                                  label=False, figsize=(10,10),levels=[acceptable_phi])\n",
    "ax.scatter(parensemble.hk,parensemble.rch_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the payoff - using our good runs, let's make some probablistic plots!  Here's our parameters\n",
    "\n",
    "### Gray blocks the full the range of the realizations.  These are within our bounds of reasonable given what we knew when we started, so those grey boxes represent our prior\n",
    "\n",
    "### The blue boxes show the runs that met our criteria, so that distribution represents our posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 7 - plot filtered and unfiltered forecast historgrams - the forecast names are available in the Pst.forecast_names attribute\n",
    "for parnme in pst.par_names:\n",
    "    ax = plt.subplot(121)\n",
    "    parensemble.loc[:,parnme].hist(bins=10,alpha=0.5,color=\"0.5\",ax=ax,normed=True)\n",
    "    parensemble.loc[good_enough,parnme].hist(bins=10,alpha=0.5,color=\"b\",ax=ax,normed=True)\n",
    "    ax.set_title(parnme)\n",
    "    ax.set_ylabel('probability')\n",
    "\n",
    "    ax = plt.subplot(122)\n",
    "    parensemble.loc[:,parnme].hist(bins=10,alpha=0.5,color=\"0.5\",ax=ax)\n",
    "    parensemble.loc[good_enough,parnme].hist(bins=10,alpha=0.5,color=\"b\",ax=ax)\n",
    "    ax.set_ylabel('count')\n",
    "\n",
    "    \n",
    "    ax.set_title(parnme)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar to the FOSM results, the future recharge (``rch_1``) is not influenced by calibration - why?\n",
    "\n",
    "## Let's look at the forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 7 - plot filtered and unfiltered forecast historgrams - the forecast names are available in the Pst.forecast_names attribute\n",
    "for forecast in pst.forecast_names:\n",
    "    ax = plt.subplot(121)\n",
    "    df_out.loc[:,forecast].hist(bins=10,alpha=0.5,color=\"0.5\",ax=ax,normed=True)\n",
    "    df_out.loc[good_enough,forecast].hist(bins=10,alpha=0.5,color=\"b\",ax=ax,normed=True)\n",
    "    ax.set_title(forecast)\n",
    "    ax.set_ylabel('probability')\n",
    "\n",
    "    ax = plt.subplot(122)\n",
    "    df_out.loc[:,forecast].hist(bins=10,alpha=0.5,color=\"0.5\",ax=ax)\n",
    "    df_out.loc[good_enough,forecast].hist(bins=10,alpha=0.5,color=\"b\",ax=ax)\n",
    "    ax.set_ylabel('count')\n",
    "\n",
    "    \n",
    "    ax.set_title(forecast)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a substaintial reduction in uncertainty (prior vs posterior) for all forecasts...great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What a power POWERFUL method to show our uncertainty, and what we learned from history matching.  Moreover, we are free of a lot of the limiting assumptions of FOSM.  \n",
    "\n",
    "(What's the downside?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's see how the forecast prior and posterior uncertainty compare to the \"truth\" (since we know the \"truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    ax = df_out.loc[:,forecast].hist(alpha=0.5,color='0.5',normed=True)\n",
    "    ax.set_yticklabels([])\n",
    "    df_out.loc[good_enough,forecast].hist(ax=ax,alpha=0.5,color='b',normed=True)\n",
    "    ax.set_title(forecast)\n",
    "    ylim = ax.get_ylim()\n",
    "    v = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([v,v],ylim,\"k--\",lw=2.0)\n",
    "    ax.set_title(forecast)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hmmm - one forecast is outside of our conditioned probabilities - that is, the posterior is overly narrow (non-conservative)...why? Our uncertainty analysis is not robust for all our observations even with MC, and we thought this was just a problem with the assumptions in FOSM!\n",
    "\n",
    "(What parameterization do we have in this notebook?  How might that help explain the problems with the travel time forecast?)\n",
    "\n",
    "#### For comparison - here are the computationally frugal FOSM uncertainty that we had from the Freyberg_k_and_r_fluxobs notebook...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"2_parameter_Freyberg_FOSM_vs_truth.jpeg\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
