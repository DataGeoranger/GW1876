{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PEST++V3_cover.jpeg\" style=\"float: left\">\n",
    "\n",
    "<img src=\"flopylogo.png\" style=\"float: right\">\n",
    "\n",
    "<img src=\"AW&H2015.png\" style=\"float: center\">\n",
    "\n",
    "# Linear/FOSM uncertainty is cool, but for the NEXT BIG THING, you gotta go Monte Carlo \n",
    "\n",
    "## (what's old is new again)\n",
    "\n",
    "As we've seen, First-Order-Second-Moment (FOSM) is quick and insightful.  But FOSM depends on an assumption that the relation between the model and the forecast uncertainty is linear.  But many times the world is nonlinear. Short cuts like FOSM need assumptions, but we can free ourselves by taking the brute force approach.  That is define the parameters that are important, provide the prior uncertainty, sample those parameters many times, run the model many times, and then summarize the results.  \n",
    "\n",
    "On a more practical standpoint, the underlying theory for FOSM and why it results in shortcomings can be hard to explain to stakeholders.  Monte Carlo, however, is VERY straightforward, its computational brute force notwithstanding. \n",
    "\n",
    "### Here's a flowchart from Anderson et al. (2015):\n",
    "\n",
    "<img src=\"Fig10.14_MC_workflow.png\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we get when we do this?  Here are results from a MODFLOW-Monte Carlo model that was built to determine the capture zone for a spring (Hunt et al., 2001). They showe that we can get average heads for all the runs:\n",
    "\n",
    "<img src=\"PB_avg_heads_Hunt2001.png\" style=\"float: center\">\n",
    "\n",
    "But even cooler - we can get a map of standard deviation of those heads:\n",
    "\n",
    "<img src=\"PB_stdev_heads_Hunt2001.png\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even cooler - the forecast for the area that provdes recharge to the spring can be related to stakeholders probabilistically!\n",
    "\n",
    "<img src=\"FigB10.4.2_MC_probablistic_capture_zone.png\" style=\"float: center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives of this notebook\n",
    "\n",
    "1) Run a real Monte Carlo run on the Freyberg model\n",
    "\n",
    "2) Look at parameter and forecast uncertainty \n",
    "\n",
    "3) Start thinking of the advantages and disadvantages of linear and nonlinear uncertainty methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick reminder of what the model looks like:\n",
    "\n",
    "It is a heterogenous 2D areal (1-layer) model that is a step up in complexity from our xsec model. Recall it looks like this:\n",
    "\n",
    "<img src=\"Freyberg_k_plot_GW_Vistas.png\" style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard two blocks needed to prep the notebook for what we cant to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import freyberg_setup as fs\n",
    "fs.setup_pest_kr(make_porosity_tpl=True)\n",
    "working_dir = fs.WORKING_DIR_KR\n",
    "pst_name = fs.PST_NAME_KR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.plot_model(working_dir, pst_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some Monte Carlo\n",
    "\n",
    "Monte Carlo uses a lots and lots of forward runs so we don't want to make the mistake of burning the silicon for a PEST control file that is not right.  Here we make doubly sure that the control file has the recharge freed (not \"fixed' in the PEST control file).  Note you can use this block in the future for your models...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "obs = pst.observation_data\n",
    "obs.loc[obs.obgnme==\"calflux\",\"weight\"] = 0.05#super subjective\n",
    "pst.write(os.path.join(working_dir,pst_name))\n",
    "pst.parameter_data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah - the recharge for the first stress period (rch_0) is fixed. But, really, by leaving the recharge for the future (rch_1) fixed we are saying that we know it perfectly when we know it even less than the the calibration period rch_0.  Being the purpose of the model is to forecast the future, in our Monte Carlo we may want to include this.  Let's use pyemu to reset both rch_0 and rch_1 (though a text editor is fine too). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.loc[[\"rch_0\",\"rch_1\"],\"partrans\"] = \"log\"\n",
    "\n",
    "\n",
    "pst.write(os.path.join(working_dir,pst_name))\n",
    "pst.parameter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good - past and future recharge are freed.  Now let's draw 500 tries from the parameter set.  \n",
    "\n",
    "Look in the command below - how does it decide the range of parameters to pull from....\n",
    "\n",
    "(this takes some time so wait for the 0 to show up below the code block before advancing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_cov = pyemu.Cov.from_parameter_data(pst,sigma_range=6)\n",
    "#mc = pyemu.MonteCarlo(pst=pst,parcov=prior_cov)\n",
    "#mc.draw(num_reals=500,enforce_bounds=\"reset\")\n",
    "parensemble = pyemu.ParameterEnsemble.from_gaussian_draw(pst=pst,cov=prior_cov,num_reals=500)\n",
    "parensemble.enforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Here's some parameter distributions you could use to control the values pulled from the parameter range (taken from Anderson et al. 2015).  In the code block you just ran we're using a Gaussian, or \"Normal\" distribution.\n",
    "\n",
    "<img src=\"Fig10.13_parameter_distribution.png\" style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Here's an example of the first 5 parameter sets of our 500 created by our draw (\"draw\" here is like \"drawing\" a card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parensemble.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distributions we are running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an aside on covariance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst1 = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "par = pst1.parameter_data\n",
    "par.loc[:,\"partrans\"] = 'fixed'\n",
    "par.loc[[\"hk\",\"rch_0\"],\"partrans\"] = \"none\"\n",
    "par.loc[\"hk\",\"parval1\"] = 30\n",
    "prior_cov = pyemu.Cov.from_parameter_data(pst1,sigma_range=8)\n",
    "prior_cov = pyemu.Cov(prior_cov.as_2d,names=prior_cov.row_names)\n",
    "c = float(prior_cov.x[0,0] * prior_cov.x[1,1] * -0.8)\n",
    "#print(c)\n",
    "prior_cov.x[0,1] = c\n",
    "prior_cov.x[1,0] = c\n",
    "#print(prior_cov)\n",
    "#mc.draw(num_reals=2000,enforce_bounds=\"reset\")\n",
    "pe = pyemu.ParameterEnsemble.from_gaussian_draw(pst=pst1,cov=prior_cov,\n",
    "                                                         use_homegrown=True,\n",
    "                                                         num_reals=2000,\n",
    "                                                         group_chunks=False)\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "ax.scatter(pe.hk,pe.rch_0)\n",
    "ax.set_xlim(par.loc[\"hk\",\"parlbnd\"],par.loc[\"hk\",\"parubnd\"])\n",
    "ax.set_ylim(par.loc[\"rch_0\",\"parlbnd\"],par.loc[\"rch_0\",\"parubnd\"])\n",
    "ax.set_xlabel(\"hk\")\n",
    "ax.set_ylabel(\"rch_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pname in pst.par_names:\n",
    "    ax = parensemble.loc[:,pname].hist(bins=20)\n",
    "    print(parensemble.loc[:,pname].min(),parensemble.loc[:,pname].max(),parensemble.loc[:,pname].mean())\n",
    "    ax.set_title(pname)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare these distributions to the upper and lower bounds of the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data\n",
    "pst.write(os.path.join(working_dir,pst_name.replace('.pst','.por.pst')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the log transformed parameters look like a log normal distribution?\n",
    "\n",
    "## Using what you just learned, now we'll make sweep through the possible combinations and distribute them across the multicores available on your laptop (just like in the response surface notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  We are going to run the forward model 500 times (!!!!) so it will take some time. \n",
    "\n",
    "using ``sweep`` - like we just talked about, watch the C= part of the YAMR progress line - this is the runs __C__ompleted. F = __F__ailed; T = __T__imed out; R = __R__unning; W = __W__aiting to run; U = __U__navailable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parensemble.to_csv(os.path.join(working_dir,\"sweep_in.csv\"))\n",
    "os.chdir(working_dir)\n",
    "pyemu.helpers.start_slaves('.',\"sweep\",pst_name.replace('.pst','.por.pst'),num_slaves=15,master_dir='.')\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alright - let's see some MC results.  For these runs, what was the Phi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.read_csv(os.path.join(working_dir,\"sweep_out.csv\"),index_col=0)\n",
    "df_out = df_out.loc[df_out.failed_flag==0,:] #drop an failed runs\n",
    "df_out.columns = [c.lower() for c in df_out.columns]\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow, some are pretty large.  Let's plot Phi for all 500 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.phi.hist(bins=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow, some of those models are really REALLY bad fits to the observations.  So when we only use our prior knowledge to sample the parameters we get a bunch of models with unacceptably Phi, and we can consider them not reasonable.  Therefore, we should NOT include them in our uncertainty analysis.\n",
    "\n",
    "\n",
    "# PAY ATTENTION:  this is super important - in this next block we are \"conditioning\" our Monte Carlo run by removing the bad runs based on a Phi cutoff. So here is the code where we are choosing which realizations we consider ``good enough`` with respect to fitting the observation data.  This is formally known as \"GLUE\" in the literature.\n",
    "\n",
    "Let's say we'll say any Phi below 2000 is acceptable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_phi = 2000.0\n",
    "good_enough = df_out.loc[df_out.phi<acceptable_phi].index.values\n",
    "print(good_enough.shape,good_enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the run number of the 500 that met this cutoff.  Let's plot just these \"good\" ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_out.phi.hist(alpha=0.5)\n",
    "df_out.loc[good_enough,\"phi\"].hist(color=\"g\",alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import response_surface as rs\n",
    "wd = os.path.join(\"..\",\"freyberg_k_and_r_response_surface\",\"freyberg_kr\")\n",
    "fig,ax = rs.plot_response_surface(parnames=[\"hk\",\"rch_0\"],\n",
    "                                  pstfile=pst_name.replace(\".pst\",\".r3.pst\"),\n",
    "                                  WORKING_DIR=wd,alpha=0.35,\n",
    "                                  label=False, figsize=(10,10),levels=[acceptable_phi])\n",
    "ax.scatter(parensemble.hk,parensemble.rch_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now the payoff - using our good runs, let's make some probablistic plots!  Here's our parameters\n",
    "\n",
    "### Gray blocks show the range of the runs we threw out.  But those were considered within our bounds when we started, so those grey boxes represent our prior\n",
    "\n",
    "### The blue boxes show the runs that met our criteria, so that distribution represents our posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for parnme in pst.par_names:\n",
    "    ax = plt.subplot(111)\n",
    "    parensemble.loc[:,parnme].hist(bins=10,alpha=0.5,color=\"0.5\",ax=ax,normed=True)\n",
    "    parensemble.loc[good_enough,parnme].hist(bins=10,alpha=0.5,color=\"b\",ax=ax,normed=True)\n",
    "    ax.set_title(parnme)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar to the FOSM results, the future recharge (``rch_1``) is not influenced by calibration - why?\n",
    "\n",
    "## Let's look at the forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    ax = df_out.loc[:,forecast].hist(alpha=0.5,color='0.5',normed=True)\n",
    "    ax.set_yticklabels([])\n",
    "    df_out.loc[good_enough,forecast].hist(ax=ax,alpha=0.5,color='b',normed=True)\n",
    "    ax.set_title(forecast)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a substaintial reduction in uncertainty (prior vs posterior) for all forecasts...great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What a power POWERFUL method to show our uncertainty, and what we learned from history matching.  Moreover, we are free of a lot of the limiting assumptions of FOSM.  \n",
    "\n",
    "(What's the downside?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's see how the forecast prior and posterior uncertainty compare to the \"truth\" (since we know the \"truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    ax = df_out.loc[:,forecast].hist(alpha=0.5,color='0.5',normed=True)\n",
    "    ax.set_yticklabels([])\n",
    "    df_out.loc[good_enough,forecast].hist(ax=ax,alpha=0.5,color='b',normed=True)\n",
    "    ax.set_title(forecast)\n",
    "    ylim = ax.get_ylim()\n",
    "    v = pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([v,v],ylim,\"k--\",lw=2.0)\n",
    "    ax.set_title(forecast)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oh-oh - some of our forecasts we are outside of our conditioned probabilities - that is, the posterior of several forecasts is overly narrow (non-conservative)...why? Our uncertainty analysis is not robust even with MC, and I thought this was just a problem with the assumptions in FOSM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
