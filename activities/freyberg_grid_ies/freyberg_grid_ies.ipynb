{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# and now for something completely different...\n",
    "\n",
    "### During these excersizes, we have seen a traditional parameter estimation then uncertainty analysis workflow.  We also saw for the pilot points notebooks, about 500 model runs to \"calibrate\" the model and then another 500 to 1000 to find a decent set (ensemble) of realizations that fit the data acceptably well.  \n",
    "\n",
    "### But...even using pilot points as a parameterization device is a form of regularization: ideally, we would have an HK parameter in every model cell. But, that is too expensive in terms of model runs for calibration and uncertainty analysis (at least as we have learned it so far).\n",
    "\n",
    "### However, there are some new techniques that free us from these computational constraints so that we can more efficiently use lots of parameters.  One approach to this is the iterative Ensemble Smoother form of the GLM.  It is implemented in pyemu and a C++ version is in the works.  Let's see how this technique works for the freyberg model - a special version of the freyberg model with an HK parameter in every cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, shutil\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import flopy as flopy\n",
    "import pyemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import freyberg_setup as fs\n",
    "fs.setup_pest_gr()\n",
    "working_dir = fs.WORKING_DIR_GR\n",
    "pst_name = fs.PST_NAME_GR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = flopy.modflow.Modflow.load(fs.MODEL_NAM,model_ws=working_dir,load_only=[\"upw\"],check=False)\n",
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "obs = pst.observation_data\n",
    "obs.loc[obs.obgnme==\"calhead\",\"weight\"] = 0.75\n",
    "par = pst.parameter_data\n",
    "hk_par = par.loc[par.pargp==\"hk\"].copy()\n",
    "hk_par.loc[:,\"i\"] = hk_par.parnme.apply(lambda x: int(x.split('_')[1][1:]))\n",
    "hk_par.loc[:,\"j\"] = hk_par.parnme.apply(lambda x: int(x.split('_')[2][1:]))\n",
    "hk_par.loc[:,\"x\"] = m.sr.xcentergrid[hk_par.i,hk_par.j]\n",
    "hk_par.loc[:,\"y\"] = m.sr.ycentergrid[hk_par.i,hk_par.j]\n",
    "hk_par.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"number of parameters: {0} : WTF!\".format(pst.npar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we create an ``EnsembleSmoother`` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pst.filename = pst_name\n",
    "ies = pyemu.EnsembleSmoother(pst=pst,num_slaves=15,slave_dir=\".\",port=4005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``EnsembleSmoother.initialize()`` does lots of things for you:\n",
    "### - make draws from parcov for the initial ``ParameterEnsemble``\n",
    "### - make draws from obscov for the \"target\" ``ObservationEnsemble``\n",
    "### - runs the initial ``ParameterEnsemble`` forward to get the initial ``ObservationEnsemble``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ies.initialize(num_reals=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what we just did was essentially an unconstrained Monte Carlo with 100 realizations - that's nothing new..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the first few ``hk`` fields - drawn from prior (uncalibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for real in ies.parensemble.index[:4]:\n",
    "    arr = np.zeros((m.nrow,m.ncol))\n",
    "    arr[hk_par.i,hk_par.j] = ies.parensemble.loc[real,hk_par.parnme]\n",
    "    m.upw.hk[0] = arr\n",
    "    m.upw.hk[0].plot(alpha=0.5,colorbar=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Those don't look very \"geologic\" - why? Answer: the Prior! (its always about the Prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize the distributions (histograms) for each of the forecasts\n",
    "\n",
    "### These distributions come from running the initial (uncalibrated) parameter ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_obs = ies.obsensemble.copy()\n",
    "for forecast in pst.forecast_names:\n",
    "    ax = ies.obsensemble.loc[:,forecast].hist(bins=10)\n",
    "    ax.set_title(forecast)\n",
    "    ylim = ax.get_ylim()\n",
    "    v = ies.pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([v,v],ylim,\"k--\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The initial (uncalibrated) phi distribution...not so good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ies.current_phi.hist(bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we only have a few observations we are trying to match, we can look at there distributions also.  The \"blue\" histogram is the results of the initial parameter ensemble evaluation.  The \"red\" is the \"target\" distribution: each observation has a unique value for each realization: the observed value + a realization of measurement noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for oname in pst.nnz_obs_names:\n",
    "    ax = ies.obsensemble_0.loc[:,oname].hist(bins=10,alpha=0.5,color='r')\n",
    "    ies.obsensemble.loc[:,oname].hist(bins=10,ax=ax,alpha=0.5,color='b')\n",
    "    ax.set_title(oname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``EnsembleSmoother.update()`` propagates the ensemble forward, updating the ``ParameterEnsemble`` through the GLM algorithm, then runs the new ``ParameterEnsemble``.  In other words, we are going to use an approximate (low fidelity) Jacobian to update the entire parameter ensemble, the we are going to run another Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ies.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how phi is doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ies.current_phi.hist(bins=10)\n",
    "plt.show()\n",
    "ies.current_phi.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice how much the ``phi`` distribution has decreased compared to the initialized ``EnsembleSmoother``: Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's run through a few more updates...and plot the phi distribution each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    ies.update()\n",
    "    phi = ies.current_phi\n",
    "    ax = plt.subplot(111)\n",
    "    phi.hist(bins=10,ax=ax)\n",
    "    ax.set_title(\"iteration:{0}, total model runs:{1}, avg phi:{2}\".format(ies.iter_num,ies.total_runs,phi.mean()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holy Crap!  phi has gotten really good after only a 400ish runs of the model - remember, there over 800 parameters. Let's see how the forecasts are doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    ax = ies.obsensemble.loc[:,forecast].hist(bins=10,color='b',alpha=0.5)\n",
    "    init_obs.loc[:,forecast].hist(bins=10,ax=ax,color='g',alpha=0.5)\n",
    "    ax.set_title(forecast)\n",
    "    ylim = ax.get_ylim()\n",
    "    v = ies.pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([v,v],ylim,\"k--\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ies.total_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = pd.read_csv(pst_name+\".iobj.csv\")\n",
    "df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "real_cols = [c for c in df_sum.columns if c.startswith(\"0\")]\n",
    "[ax.plot(df_sum.total_runs,df_sum.loc[:,rc],'0.5',lw=0.25) for rc in real_cols]\n",
    "ax.plot(df_sum.total_runs,df_sum.loc[:,\"mean\"],\"k\",lw=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Awesome!  We are crushing phi...but how to the parameter fields looks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for real in ies.parensemble.index[:4]:\n",
    "    arr = np.zeros((m.nrow,m.ncol))\n",
    "    arr[hk_par.i,hk_par.j] = ies.parensemble.loc[real,hk_par.parnme]\n",
    "    m.upw.hk[0] = arr\n",
    "    m.upw.hk[0].plot(alpha=0.5,colorbar=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uh oh. The fields look like noise...how can we fix this? Solution: a full covariance matrix that expresses spatial correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iES with a full covariance matrix\n",
    "\n",
    "## Now let's rerun the iES process but with a full, geostatistical prior covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=1.0,a=2500.0)\n",
    "gs = pyemu.geostats.GeoStruct(variograms=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov = pyemu.helpers.geostatistical_prior_builder(pst=pst,struct_dict={gs:[hk_par]},sigma_range=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how this covariance looks compare to the one we used previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cov.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ies.parcov.as_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create a new ``iES`` and update 3 times..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ies = pyemu.EnsembleSmoother(pst=pst,num_slaves=15,slave_dir=\".\",parcov=cov,port=4005)\n",
    "ies.initialize(num_reals=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visual the new parameter fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for real in ies.parensemble.index[:4]:\n",
    "    arr = np.zeros((m.nrow,m.ncol))\n",
    "    arr[hk_par.i,hk_par.j] = ies.parensemble.loc[real,hk_par.parnme]\n",
    "    m.upw.hk[0] = arr\n",
    "    m.upw.hk[0].plot(alpha=0.5,colorbar=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Those fields look much more \"geologic\" (what ever that means)...let's see how well the smoother does with these fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "    ies.update()\n",
    "    phi = ies.current_phi\n",
    "    ax = plt.subplot(111)\n",
    "    phi.hist(bins=10,ax=ax)\n",
    "    ax.set_title(\"iteration:{0}, total model runs:{1}\".format(ies.iter_num,ies.total_runs))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### phi looks really good still...let's see how the final (calibrated) parameter fields look...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for real in ies.parensemble.index[:4]:\n",
    "    arr = np.zeros((m.nrow,m.ncol))\n",
    "    arr[hk_par.i,hk_par.j] = ies.parensemble.loc[real,hk_par.parnme]\n",
    "    m.upw.hk[0] = arr\n",
    "    m.upw.hk[0].plot(alpha=0.5,colorbar=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    ax = ies.obsensemble.loc[:,forecast].hist(bins=10,color='b',alpha=0.5,label=\"posterior\")\n",
    "    init_obs.loc[:,forecast].hist(bins=10,color=\"0.5\",alpha=0.5,label=\"prior\")\n",
    "    ax.set_title(forecast)\n",
    "    ylim = ax.get_ylim()\n",
    "    v = ies.pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([v,v],ylim,\"k--\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that the final (posterior) ensemble is bracketing the \"truth\" for all forecasts...yeah! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
