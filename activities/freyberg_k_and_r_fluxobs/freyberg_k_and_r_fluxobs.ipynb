{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PEST++V3_cover.jpeg\" style=\"float: left\">\n",
    "\n",
    "<img src=\"flopylogo.png\" style=\"float: right\">\n",
    "\n",
    "<img src=\"AW&H2015.png\" style=\"float: center\">\n",
    "\n",
    "# History match the Freyberg model using a two parameters ``K`` and ``R`` using head and flux observations\n",
    "\n",
    "#### Where are we on the Goldilocks complexity curve? \n",
    "\n",
    "<img src=\"Hunt1998_sweetspot.png\" style=\"float: center\">\n",
    "\n",
    "\n",
    "\n",
    "The runs so far were intended to be greatly oversimplified so as to be a starting point for adding complexity. However, when we added just __*one more parameter*__ for a total of 2 parameters uncerainty for some forecasts got appreciably __worse__.  And these parameters cover the entire model domain, which is unrealistic for the natural world!  Are we past the \"sweetspot\" and should avoid any additional complexity even if our model looks nothing like reality?  \n",
    "\n",
    "Adding parameters in and of itself is not the real problem.  Rather, it is adding parameters that influence forecasts but are unconstrained by observations so that they are free to wiggle and ripple uncertainty to our forcasts.  If observations are added that help constrain the parameters, the forecast observation will be more certain. That is, the natural flip side of adding parameters is constraining them, with data (first line of defense) or soft-knowledge and problem dimension reduciton (SVD).  \n",
    "\n",
    "Anderson et al. (2015) suggest that at a minimum groundwater models be history matched to heads and fluxes.  There is a flux observation in our PEST control file, but it was given zero weight.  Let's see what happens if we move our model to the minimum calibration of Anderson et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives for this notebook are to:\n",
    "\n",
    "1) Add a flux observation to the measurement objective function of our Freyberg model\n",
    "\n",
    "2) Explore the effect of adding the observation to history matching, parameter uncertainty, and forecast uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import freyberg_setup as fs\n",
    "pst_name = fs.PST_NAME_KR\n",
    "working_dir = fs.WORKING_DIR_KR\n",
    "fs.setup_pest_kr()\n",
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "pst.control_data.noptmax = 0\n",
    "pst.observation_data.loc[pst.observation_data.obgnme=='calflux', 'weight']=0\n",
    "pst.write(os.path.join(working_dir,pst_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.plot_model(working_dir, pst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.run(\"pestpp {0}\".format(pst_name),cwd=working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``PEST++`` only ran the model one time - because NOPTMAX=0, but that is what we want at this point to interrogate the model at initial values.  \n",
    "\n",
    "### Let's look at all observations in the PEST run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "pst.observation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow!  that's a lot of observations.  Why so many?  Answer:  we are \"carrying\" lots of model outputs that may be of interest to us later __(not just places and times where we have actual measurements)__.  These outputs include forecasts as well as *\"potential\" observation* locations we will use in dataworth analysis (more on that later)\n",
    "\n",
    "But, the calibration only uses observations where you assign weights.  Let's get a listing of just those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.nnz_obs_names,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give the observation ``rivflux_cal`` a non-zero weight.  You can do this in a text editor but we'll do it in the next block and see the report out for convenience. We chose a new weight of 0.05 for this problem, but we'll spend more time on the concepts involved with observation weighting in a later notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "obs.loc[obs.obgnme==\"calflux\",\"weight\"] = 0.05 #super subjective\n",
    "pst.observation_data.loc[pst.nnz_obs_names,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's change NOPTMAX from 0 to 20 so we can see what the effect of weighting the flux target is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 20\n",
    "pst.write(os.path.join(working_dir,pst_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And we'll run the model - look at the terminal window where you launched this notebook to see the progress of PEST++.  Advance through the code blocks when you see a 0 returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.run(\"pestpp {0}\".format(pst_name),cwd=working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the results, how did we do with fit (lowering PHI)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj = pd.read_csv(os.path.join(working_dir,pst_name.replace(\".pst\",\".iobj\")),index_col=0)\n",
    "df_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Egads!  Our Phi is a bit larger!  Are we moving backwards? Oh wait, we added a new weighted observation, so we can't compare it directly to what we had with only head observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Okay, what did it do to our parameter uncertainty?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paru = pd.read_csv(os.path.join(working_dir,pst_name.replace(\".pst\",\".par.usum.csv\")),index_col=0)\n",
    "df_paru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hold the phone - only K is showing here.  Did we run PESTCHEK before burning the silicon? \n",
    "\n",
    " (Remember last notebook where we said:  \"Let's run PESTCHEK and see what it says about our freyberg.pst file\"?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.run(\"pestchek {0}\".format(pst_name),cwd=working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Well the instructors gave you the same PEST control file as last exerisce!  Some one should tell them that it was \"curious\" in the last notebook but vexxing now, because we again see that in the PESTCHEK warning section it says \"All parameters belonging to the parameter group \"rch\" are either fixed or tied\". That is flagged as a warning because PESTCHEK is wondering (with good reason in this case) why would it not be adjustable after you went to all the trouble to define it as a parameter.  But, there  are times you may want to do this, so it is classified as a warning and isn't going to stop you.\n",
    "\n",
    "#### But that is not what we want, we want to make recharge a parameter in this activity and redo our work (did we remember to mention the importance of running PESTCHEK?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the PEST control file freyberg.pst in your text editor.  \n",
    "\n",
    "1) Look in the parameter data section\n",
    "\n",
    "2) Find the parameter __rch_0__ (the recharge for the calibration period) and make it adjustable (hint:  look at the other parameters) \n",
    "\n",
    "3) Save the file\n",
    "\n",
    "4) Run PESTCHEK on the PEST control file in a seperate terminal window or by executing the next code block and looking at the terminal window where you launched this notebook\n",
    "\n",
    "## Or run this next pyemu block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.loc[\"rch_0\",'partrans'] = \"log\"\n",
    "pst.write(os.path.join(working_dir,pst_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.run(\"pestchek {0}\".format(pst_name),cwd=working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to redo our steps from above....look at the terminal window where you launched this notebook to see the progress of PEST++.  Advance through the code blocks when you see a 0 returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.run(\"pestpp {0}\".format(pst_name),cwd=working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Again, let's look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj = pd.read_csv(os.path.join(working_dir,pst_name.replace(\".pst\",\".iobj\")),index_col=0)\n",
    "df_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### not much improvement in phi from before,  what's the point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paru = pd.read_csv(os.path.join(working_dir,pst_name.replace(\".pst\",\".par.usum.csv\")),index_col=0)\n",
    "df_paru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Much better - thanks PESTCHEK.  Yes, RCH_0 has a mean of zero but remember we log transformed it and it is reported in log space.  \n",
    "\n",
    "### Now let's compare the parameter uncertainty results with the flux observation above to the previous run where we zero weighted the flux observation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paru_base = pd.read_csv(os.path.join(\"..\",\"freyberg_k_and_r\",\n",
    "                                        working_dir,pst_name.replace(\".pst\",\".par.usum.csv\")),index_col=0)\n",
    "df_paru_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Uncertainty is lower when we add an observation that can constrain the parameters.  \n",
    "\n",
    "### Here's the parameter uncertainty for the K and R parameters, side by side, heads+flux observation vs heads only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paru_concat = pd.concat([df_paru,df_paru_base],join=\"outer\",axis=1,keys=[\"heads+fluxobs\",\"heads_only\"])\n",
    "df_paru_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting - a tradeoff with fit between the two types of observations...\n",
    "\n",
    "\n",
    "###  Let's plot these up like before.  Here's the prior and posterior standard deviations (blue is with flux observation weighted, green is with zero weight on the flux target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs,axes = pyemu.helpers.plot_summary_distributions(df_paru,subplots=True)\n",
    "for pname,ax in zip(pst.adj_par_names,axes):\n",
    "    pyemu.helpers.plot_summary_distributions(df_paru_base.loc[[pname.upper()],:],ax=ax,pt_color=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wow, from an uncertainty standpoint, the flux observation has helped us learn a lot about recharge, and the uncertainty in HK improved to (blue posterior is narrower and higher than green)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's look at our forecasts - here's the K and R model with the flux observation with weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foreu = pd.read_csv(os.path.join(working_dir,pst_name.replace(\".pst\",\".pred.usum.csv\")),index_col=0)\n",
    "df_foreu.loc[:,\"reduction\"] = 100.0 *  (1.0 - (df_foreu.post_stdev / df_foreu.prior_stdev))\n",
    "\n",
    "df_foreu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare these results with the ``k_and_r`` model *without* the flux observation (below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foreu_base = pd.read_csv(os.path.join(\"..\",\"freyberg_k_and_r\",\n",
    "                                         working_dir,pst_name.replace(\".pst\",\".pred.usum.csv\")),index_col=0)\n",
    "df_foreu_base.loc[:,\"reduction\"] = 100.0 *  (1.0 - (df_foreu_base.post_stdev / df_foreu_base.prior_stdev))\n",
    "df_foreu_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And here the forecast uncertianties are side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foreu_concat = pd.concat([df_foreu,df_foreu_base],join=\"outer\",axis=1,keys=[\"heads+fluxobs\",\"heads_only\"])\n",
    "df_foreu_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### and plotted ( (blue is with flux observation weighted, green is with zero weight on the flux target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in pst.forecast_names:\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122)\n",
    "    pyemu.helpers.plot_summary_distributions(df_foreu.loc[[forecast.upper()],:],ax=ax1)\n",
    "    pyemu.helpers.plot_summary_distributions(df_foreu_base.loc[[forecast.upper()],:],\n",
    "                                             ax=ax2,pt_color='g')\n",
    "    xlim1, xlim2 = ax1.get_xlim(),ax2.get_xlim()\n",
    "    xmx = max(xlim1[1],xlim2[1])\n",
    "    xmn = min(xlim1[0],xlim2[0])\n",
    "    ax1.set_xlim(xmn,xmx)\n",
    "    ax2.set_xlim(xmn,xmx)\n",
    "    ylim1, ylim2 = ax1.get_ylim(),ax2.get_ylim()  \n",
    "    ymx = max(ylim1[1],ylim2[1])\n",
    "    ymn = min(ylim1[0],ylim2[0])\n",
    "    ax1.set_ylim(ymn,ymx)\n",
    "    ax2.set_ylim(ymn,ymx)\n",
    "    \n",
    "    \n",
    "    ax1.set_title(forecast)\n",
    "    ax2.set_title(\"without flux obs\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in df_foreu_concat.index:\n",
    "    df_foreu_concat.loc[forecast,(slice(None),(\"prior_stdev\",\"post_stdev\"))].plot(kind=\"bar\",subplots=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced digression - what's wrong with FOSM...\n",
    "### Why are the Prior distributions for the flux obs and no flux obs forecasts different? - I thought Priors should be the same between these two?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jco = pyemu.Jco.from_binary(os.path.join(working_dir,pst_name.replace(\".pst\",\".jcb\")))\n",
    "\n",
    "df = jco.to_dataframe()\n",
    "\n",
    "df.loc[\"travel_time\",:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jco = pyemu.Jco.from_binary(os.path.join(\"..\",\"freyberg_k_and_r\",working_dir,pst_name.replace(\".pst\",\".jcb\")))\n",
    "\n",
    "df2 = jco.to_dataframe()\n",
    "df2.loc[\"travel_time\",:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### well the parameter sensitivity is different at these different optimal values.... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The information in the flux obs has reduced river flux forecast uncertainty dramatically, but has not really improved what we had before with respect to the ``travel_time`` or head forecasts.  So on first blush we see that the same model/observation data set can make some forecasts better but not others\n",
    "\n",
    "### But there is more to it than that - think about which observation helped which parameter and which forecast the most.  Is there an \"birds of the feather\" type of thing going on?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay, we took you a long way but doesn't our approach to uncertainty have a huge flaw?  We freed recharge rch_0 which is the calibration period that we know something about.  But we left the recharge in the forecast period (rch_1) fixed - which is saying that we know it perfectly, which makes no sense.  This set of blocks will recreate the steps above and get us back to the parameter and forecast uncertainty plots...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.parameter_data.loc[[\"rch_0\",\"rch_1\"],\"partrans\"] = \"log\"\n",
    "pst.write(os.path.join(working_dir,pst_name))\n",
    "pst.parameter_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.run(\"pestchek {0}\".format(pst_name),cwd=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.helpers.run(\"pestpp {0}\".format(pst_name),cwd=working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_obj = pd.read_csv(os.path.join(working_dir,pst_name.replace(\".pst\",\".iobj\")),index_col=0)\n",
    "df_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paru_f = pd.read_csv(os.path.join(working_dir,pst_name.replace(\".pst\",\".par.usum.csv\")),index_col=0)\n",
    "df_paru_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs,axes = pyemu.helpers.plot_summary_distributions(df_paru_f,subplots=True)\n",
    "for pname,ax in zip(pst.adj_par_names,axes):\n",
    "    if pname.upper() == \"RCH_1\":\n",
    "        continue\n",
    "    pyemu.helpers.plot_summary_distributions(df_paru_base.loc[[pname.upper()],:],ax=ax,pt_color=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ah, for the future recharge there is not change between the prior and posterior uncertainty, but this makes sense because there are no weighted observations (information) regarding the future. So our calibration data tells us nothing about the this parameter.  \n",
    "\n",
    "## How does this new source of uncertainty ripple to our forecast uncertainty?\n",
    "\n",
    "# Jeremy - can you fix this so it plots the rch_1 fixed vs freed change in FOSM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foreu_f = pd.read_csv(os.path.join(working_dir,pst_name.replace(\".pst\",\".pred.usum.csv\")),index_col=0)\n",
    "df_foreu_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs,axes = pyemu.helpers.plot_summary_distributions(df_foreu_f,subplots=True)\n",
    "for forecast,ax in zip(pst.forecast_names,axes):\n",
    "    pyemu.helpers.plot_summary_distributions(df_foreu.loc[[forecast.upper()],:],ax=ax,pt_color=\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jco = pyemu.Jco.from_binary(os.path.join(working_dir,pst_name.replace(\".pst\",\".jcb\")))\n",
    "df = jco.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[pst.forecast_names,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foreu_concat = pd.concat([df_foreu,df_foreu_f],join=\"outer\",axis=1,keys=[\"base\",\"rch_1\"])\n",
    "df_foreu_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_foreu_concat.sort_index(axis=1,inplace=True)\n",
    "for forecast in df_foreu_concat.index:\n",
    "    df_foreu_concat.loc[forecast,(slice(None),(\"prior_stdev\",\"post_stdev\"))].plot(kind=\"bar\",subplots=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see that the posterior for most forecasts is increased because of including future recharge uncertainty.  Intutitively, it makes sense because future recharge directly influences water levels and fluxes in the future.  And since calibration (history-matching) can't tell us anything about future recharge.  This means there is no data we can collect to reduce this source of uncertainty...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
