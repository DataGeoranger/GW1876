{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Rectangle as rect\n",
    "import matplotlib.pyplot as plt\n",
    "import flopy\n",
    "import pyemu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo with Freyberg Pilot Points Model\n",
    "\n",
    "### This is essentially the same work flow as ``freyberg_kr`` and ``freyberg_zone`` monte carlo notebooks - again, the power of scripting!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `pyemu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import freyberg_setup as fs\n",
    "fs.setup_pest_pp()\n",
    "working_dir = fs.WORKING_DIR_PP\n",
    "pst_name = fs.PST_NAME_PP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.plot_model(working_dir, pst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = pyemu.MonteCarlo(pst=pst,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important: since we didn't pass a ``parcov`` arg to ``MonteCarlo``, that means the prior parameter covariance matrix was constructed from the parameter bounds (it does NOT include spatial correlation information for the pilot points - remember all the geostatics?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mc.parcov.as_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```draw```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.draw(num_reals=500)\n",
    "print(mc.parensemble.shape)\n",
    "print(mc.parensemble.mean().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting\n",
    "\n",
    "### Since ```ParameterEnsemble``` is dervied from ```pandas.DataFrame```, it has all the cool methods and attributes we all love.  Let's compare the results of drawing from a uniform vs a gaussian distribution.  This may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc.parensemble.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "partoplot = 'hk10'\n",
    "mc.parensemble.loc[:,partoplot].plot(kind=\"hist\",bins=50,ax=ax,alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_first_10_reals(paren):\n",
    "    arrs = []\n",
    "    pp_df = pyemu.gw_utils.pp_tpl_to_dataframe(os.path.join(working_dir,\"hkpp.dat.tpl\"))\n",
    "    pp_df.loc[:,\"iidx\"] = pp_df.index\n",
    "    for real in paren.index.values[:10]:\n",
    "        pp_df.index = pp_df.parnme\n",
    "        pp_df.loc[:,\"parval1\"] = paren.loc[real,pp_df.parnme].T\n",
    "        pp_df.index = pp_df.iidx\n",
    "        arr = pyemu.utils.gw_utils.fac2real(pp_df,os.path.join(working_dir,\"hkpp.dat.fac\"))\n",
    "        arrs.append(np.log10(np.loadtxt(arr)))\n",
    "    mx,mn = -1.0E+10,1.0e+10\n",
    "    for arr in arrs:\n",
    "        mx = max(mx,arr.max())\n",
    "        mn = min(mn,arr.min())\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    m = flopy.modflow.Modflow.load(fs.MODEL_NAM,model_ws=working_dir)\n",
    "    for i,arr in enumerate(arrs):\n",
    "        ax = plt.subplot(2,5,i+1,aspect=\"equal\")\n",
    "        m.upw.hk[0] = arr\n",
    "        m.upw.hk[0].plot(axes=[ax],alpha=0.5)\n",
    "        ax.scatter(pp_df.x,pp_df.y,marker='.',color='k',s=4)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_10_reals(mc.parensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Do these look \"right\" (from a geologic stand point)? Lots of \"random\" variation (pilot points spatially near each other can have very different values)...not much structure...why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use a full covariance matrix and see how that looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(contribution=1.0,a=2500,anisotropy=1.0,bearing=0.0)\n",
    "gs = pyemu.utils.geostats.GeoStruct(variograms=[v])\n",
    "pp_tpl = os.path.join(working_dir,\"hkpp.dat.tpl\")\n",
    "cov = pyemu.helpers.geostatistical_prior_builder(pst=mc.pst,struct_dict={gs:pp_tpl})\n",
    "plt.imshow(cov.x,interpolation=\"nearest\")\n",
    "cov.to_dataframe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_full = pyemu.MonteCarlo(pst=pst,parcov=cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_full.draw(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_10_reals(mc_full.parensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Much better! now let's run the parameter ensemble through the model using sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_full.parensemble.to_csv(os.path.join(working_dir,\"sweep_in.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember, this is going to give your machine a work out....watch what is going on the terminal and wait for the \"*\" to become a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.chdir(working_dir)\n",
    "pyemu.helpers.start_slaves('.',\"sweep\",pst_name,num_slaves=15,master_dir='.')\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.read_csv(os.path.join(working_dir,\"sweep_out.csv\"),index_col=0)\n",
    "df_out.columns = [c.lower() for c in df_out.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_out = df_out.loc[df_out.failed_flag==0,:] #drop an failed runs\n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So now we need to specify our \"good enough\" threshold.  Look back at your ``freyberg_pilot_points_2\\freyberg_pilot_points_setup`` notebook and use a value slightly larger than your ``phimlim`` value...makes sense right?  We should expect \"good enough\" realizations to atleast be in the ball park of the target objective function...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_phi = 550.0\n",
    "good_enough = df_out.loc[df_out.phi<acceptable_phi].index.values\n",
    "print(good_enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oh crap! what happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.phi.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.phi.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is a major problem with \"rejection sampling\" in high dimensions: you have to run the model many many many many many times to find even a few realizations that fit the data acceptably well.  \n",
    "\n",
    "### With all these parameters, there are so many possible combinations, that very few realizations fit the data very well...we will address this problem later, so for now, let bump our \"good enough\" threshold to some realizations to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_phi = 1500.0\n",
    "good_enough = df_out.loc[df_out.phi<acceptable_phi].index.values\n",
    "print(good_enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in mc.pst.forecast_names:\n",
    "    ax = df_out.loc[:,forecast].hist(alpha=0.5,color='0.5',normed=True)\n",
    "    ax.set_yticklabels([])\n",
    "    df_out.loc[good_enough,forecast].hist(ax=ax,alpha=0.5,color='b',normed=True)\n",
    "    ax.set_title(forecast)   \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting results!!! We see that for the river flux and travel time forecasts, the posterior uncertainty is very similar to the prior...that means \"calibration\" hasn't helped us learn about these forecasts...\n",
    "\n",
    "### As before, let's now use our knowledge of the \"truth\" to see how we are doing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for forecast in mc.pst.forecast_names:\n",
    "    ax = df_out.loc[:,forecast].hist(alpha=0.5,color='0.5',normed=True)\n",
    "    ax.set_yticklabels([])\n",
    "    df_out.loc[good_enough,forecast].hist(ax=ax,alpha=0.5,color='b',normed=True)\n",
    "    ax.set_title(forecast)   \n",
    "    ylim = ax.get_ylim()\n",
    "    v = mc.pst.observation_data.loc[forecast,\"obsval\"]\n",
    "    ax.plot([v,v],ylim,\"k--\",lw=2.0)\n",
    "    ax.set_title(forecast)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Its hard to say how the posterior compares to the prior with so few \"good enough\" realizations.  To fix this problem, we have two choices:\n",
    "### - run the model more times for Monte Carlo (!)\n",
    "### - generate realizations that fix the data better before hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
