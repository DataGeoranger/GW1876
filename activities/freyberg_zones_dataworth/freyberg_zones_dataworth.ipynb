{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataworth with Freyberg Zones Model\n",
    "\n",
    "Here we are going to investigate the sources of forecast uncertainty as well as the worth of observations to reduce forecast uncertainty.  Important things to remember:\n",
    "- FOSM doesn't require values for observations, parameters or forecast.  We do need sensitivities between the quantities and estimates of prior uncertainties for parameters and observtion noise.  \n",
    "- Zonal boundaries are baked into the problem. Are they really perfectly known? Does it matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import flopy\n",
    "import pyemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import freyberg_setup as fs\n",
    "fs.setup_pest_zn()\n",
    "working_dir = fs.WORKING_DIR_ZN\n",
    "pst_name = fs.PST_NAME_ZN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.plot_model(working_dir, pst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(working_dir,pst_name))\n",
    "pst.control_data.noptmax = -1\n",
    "pst.write(os.path.join(working_dir,pst_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(working_dir)\n",
    "pyemu.helpers.start_slaves('.',\"pestpp\",pst_name,num_slaves=5,master_dir=\".\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sc = pyemu.Schur(os.path.join(working_dir,pst_name.replace(\".pst\",\".jcb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pst.parameter_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick check of which parameters are being informed by the observations during calibration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = plt.subplot(111)\n",
    "sc.get_parameter_summary().percent_reduction.plot(kind=\"bar\",ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.get_parameter_summary().percent_reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter contribution analysis\n",
    "\n",
    "We are going to use the ``Schur.get_par_contribution()`` method. It calculates the *decrease* in forecast uncertainty resulting for hypothetical \"perfect knowledge\" (no uncertainty) of arbitrary groups of parameters. If you don't pass any arguments, then it tests each parameter independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_pc = sc.get_par_contribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pc = 100.0 * (1.0 - df_pc / df_pc.loc[\"base\",:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for forecast in df_pc.columns:\n",
    "    ax = df_pc.sort_values(by=forecast,ascending=False).loc[:,forecast].iloc[:10].plot(kind=\"bar\")\n",
    "    ax.set_title(forecast)\n",
    "    ax.set_ylabel(\"percent reduction in uncertainty\")\n",
    "    ax.set_xlabel(\"parameter\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a mixture of HK zones and past and future recharge...so we see that each forecast depends on (potentailly) different combinations of parameters.  Does this mean we can still make a model that makes several kinds of forecasts???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation worth\n",
    "\n",
    "Let's see what observations are most important to the forecasts.  We will start be investigating the value of the existing observations using the ``Schur.get_removed_obs_importance()`` method.  It calculates the *increase* in forecast uncertainty that happens if we lose each existing observation. We will start with the existing observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_rm = sc.get_removed_obs_importance()\n",
    "df_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rm = 100.0 * (1.0 - df_rm.loc[\"base\",:] / df_rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for forecast in df_rm.columns:\n",
    "    ax = df_rm.sort_values(by=forecast,ascending=False).loc[:,forecast].iloc[:10].plot(kind=\"bar\")\n",
    "    ax.set_title(forecast)\n",
    "    ax.set_ylabel(\"percent increase in uncertainty\")\n",
    "    ax.set_xlabel(\"parameter\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the value of yet-to-be-collected observations\n",
    "\n",
    "## Using the assumptions of FOSM, we can also calculate the value of observations we don't have yet!  Black magic!  Not really, we carried these possible data locations as ``observations`` in the control file - this means each one has a row in the jacobian (just like any other output of interest).  If we then assume some value of measurement noise (a non-zero weight in the control file), we can run a hypothetical \"calibration\" analysis using Schur compliment.  Remember, FOSM doesn't care about actual values of parameters, observations, or forecasts, only sensitivities...\n",
    "\n",
    "## In pyemu, the ``Schur.get_added_obs_importance()`` method calculates the *decrease* in uncertainty resulting for having new obsevations during calibration.  It accepts an optional argument ``obslist_dict`` which is a python dictionary of groups of new observations to test.  Here we will test the value of having the water level in each unmeasured model cell.  Then we can make maps of where to collect new data...these calculations take a little while to run because we are repeatedly calculating the entire FOSM process for each new observation location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pobs_names = [o for o in sc.pst.obs_names if \"pr\" in o]\n",
    "df_ad = sc.get_added_obs_importance(obslist_dict={oname:oname for oname in pobs_names},\n",
    "                                base_obslist=sc.pst.nnz_obs_names,reset_zero_weight=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pobs_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_ad.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_ad.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just a helper function to make maps of data worth\n",
    "m = flopy.modflow.Modflow.load(fs.MODEL_NAM,model_ws=working_dir)\n",
    "obs = pst.observation_data\n",
    "hobs = obs.loc[obs.obgnme==\"calhead\",:].copy()\n",
    "hobs.loc[:,\"i\"] = hobs.obsnme.apply(lambda x: int(x[6:8])-1)\n",
    "hobs.loc[:,\"j\"] = hobs.obsnme.apply(lambda x: int(x.split('_')[0].split('c')[-1])-1)\n",
    "hobs.loc[:,\"x\"] = m.sr.xcentergrid[hobs.i,hobs.j]\n",
    "hobs.loc[:,\"y\"] = m.sr.ycentergrid[hobs.i,hobs.j]\n",
    "wdata = pd.DataFrame.from_records(m.wel.stress_period_data[0])\n",
    "wdata.loc[:,\"x\"] = m.sr.xcentergrid[wdata.i,wdata.j]\n",
    "wdata.loc[:,\"y\"] = m.sr.ycentergrid[wdata.i,wdata.j]\n",
    "def plot_added_worth(df):\n",
    "    df_base = df.loc[\"base\",:]\n",
    "    df_frac = 100.0 * (1.0 - (df / df_base))\n",
    "    df_frac = df_frac.loc[pobs_names,:]\n",
    "    df_frac.loc[:,\"i\"] = df_frac.index.map(lambda x: int(x[6:8])-1)\n",
    "    df_frac.loc[:,\"j\"] = df_frac.index.map(lambda x: int(x[9:11])-1)\n",
    "    zn_arr = np.loadtxt(os.path.join(working_dir,\"hk.zones\"))\n",
    "    for forecast in sc.pst.forecast_names:\n",
    "        print(forecast)\n",
    "        arr = np.zeros((m.nrow,m.ncol)) - 1\n",
    "        arr[df_frac.i,df_frac.j] = df_frac.loc[:,forecast]\n",
    "        arr = np.ma.masked_where(arr < 0.0 ,arr)\n",
    "        fig = plt.figure(figsize=(12,10))\n",
    "        \n",
    "        ax = plt.subplot(122,aspect=\"equal\")\n",
    "        c = ax.pcolormesh(m.sr.xedge,m.sr.yedge,arr,alpha=0.75)\n",
    "        plt.colorbar(c)\n",
    "        ax2 = plt.subplot(121,aspect=\"equal\")\n",
    "        \n",
    "        c = ax2.pcolormesh(m.sr.xedge,m.sr.yedge,zn_arr)\n",
    "        plt.colorbar(c)\n",
    "#         if forecast.startswith(\"fr\"):\n",
    "#             i = int(forecast[2:4]) - 1\n",
    "#             j = int(forecast.split('_')[0].split('c')[-1]) - 1\n",
    "#             x,y = m.sr.xcentergrid[i,j],m.sr.ycentergrid[i,j]\n",
    "#             ax.scatter([x],[y],marker='o',color='r',s=150)\n",
    "#             ax2.scatter([x],[y],marker='o',color='r',s=150)    \n",
    "        ax.scatter(hobs.x,hobs.y,marker='x',s=100,color='y')\n",
    "        ax.scatter(wdata.x,wdata.y,marker='*',s=100,color='m')\n",
    "        ax2.scatter(hobs.x,hobs.y,marker='x',s=100,color='y')\n",
    "        ax2.scatter(wdata.x,wdata.y,marker='*',s=100,color='m')\n",
    "        ax.set_title(forecast)\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to plot the worth (reduction in uncertainty) of a single, new water level measurement for each forecast (forecast name is the title of each plot). So these plots show us where to go collect new data to reduce forecast uncertainty as much as possible...red circles show the forecast (for water level forecasts), yellow X's are the locations of existing water levels, purple stars mark pumping well locations (pumping rates are treated as uncertain!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_added_worth(df_ad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are kinda weird, right?  In many ways, these dataworth results seem to not follow common sense/physical meaning...why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
