\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=1.5cm,bmargin=1.5cm,lmargin=1.5cm,rmargin=1.5cm}

\begin{document}
\title{Glossary of important terms for GW1876}
\maketitle

\section*{Notes on mathematics symbology}
It would be really nice if a totally consistent set of symbols was used in the mathematics of parameter estimation and uncertainty analysis. But....no such luck! So many researchers in different fields over a long time have contributed to the work used in this class. As a result, this glossary is an attempt to highlight some general symbology and clarify some terms.

First of all, though, one thing that is consistent (mostly!) is the general linear algebra notation used throughout the class.
\begin{description}
\item[scalar values] Lowercase, non-bold font indicates a scalar (single) value: $x$, $y$, $z$
\item[vectors] Lowercase, bold font indicates a vector of values: $\mathbf{x}$, $\mathbf{y}$, $\mathbf{z}$ 
\item[matrices] Uppercase, bold font indicates a matrix of values: $\mathbf{X}$, $\mathbf{Q}$, $\mathbf{J}$ A matrix with $<\cdot>^{T}$ indicates a matrix transpose. A matrix with $<\cdot>^{-1}$ indicates a matrix inverse.
\item[matrix multiplication] Then, matrix multiplication (with either other matrices or vectors) is expressed simply by adjacent matrices: $\mathbf{Xy}$, $\mathbf{X}^T\mathbf{Q}^{-1}\mathbf{X}$
\end{description}
\section*{Glossary of terms and equations}
\begin{description}
\item [Parameters] Variable input values for models, typically representing system properties and forcings. Values to be estimated in the history matching process. Typically identified as $k$, $p$, or $x$ ($\mathbf{k}$, $\mathbf{p}$ or $\mathbf{x}$ for multiple parameters in a vector).
\item [Observation] Measured system state values. These values are used to compare with model outputs collocated in space and time. The term is often used to mean \emph{both} field measurements and outputs from the model. When referring to a measured value, observations are typically identified by the variables $y$ or $o$  ($\mathbf{y}$ or $\mathbf{o}$ for multiple parameters in a vector)
\item [Modeled Equivalent] A modeled value collocated in time and space with an observation. There are various ways to identify a single or multiple modeled equivalent values (and, to make things confusing, they are often \emph{also} called ``observations"!)  \newline{}
\textbf{Single values} 
\begin{enumerate}
\item $f\left(x\right)$
\item $X\left(\beta\right)$
\item $M\left(p\right)$
\end{enumerate}
\textbf{Multiple values}
\begin{enumerate}
\item $\mathbf{X}\beta$
\item $\mathbf{M}\mathbf{p}$
\item $\mathbf{NOBS}$ Number of observations/simulated equivalents in the inverse model setup
\item $\mathbf{NPAR}$ Number of adjustable input parameters in the inverse model setup

\end{enumerate}
\item [Forecasts] Model outputs for which field observations are not available. Typically these values are simulated under an uncertain future condition.
\item [Phi] Objective function, defined as the weighted sum of squares of residuals. Phi (aka $\Phi$) is typically calculated as
\begin{equation}
\begin{array}{ccc}
 \Phi=\sum_{i=1}^{n}\left(\frac{y_{i}-f\left(x_{i}\right)}{w_{i}}\right)^{2} & or & \Phi=\left(\mathbf{y}-\mathbf{Jx}\right)^{T}\mathbf{Q}^{-1}\left(\mathbf{y}-\mathbf{Jx}\right)
 \end{array}
\end{equation}
\item [Residuals] The difference between observation values and modeled equivalents $r_i=y_i-f\left(x_i\right)$
\item [Sensitivity] The incremental change of an observation (modeled equivalent, actually) due to an incremental change in a parameter. Typically expressed as a finite-difference approximation of a partial derivative: $\frac{\partial y}{\partial x}$
\item [Jacobian Matrix] A matrix of the sensitivity of all observations in an inverse model to all parameters. This is often shown as a matrix by various names $\mathbf{X}$, $\mathbf{J}$, or $\mathbf{H}$. Each element of the matrix is a single sensitivity value  $\frac{\partial y_i}{\partial x_j}$ for $i\in NOBS$, $j \in NPAR$
\item [Regularization] A preferred condition pertaining to parameters, the deviation from which, elicits a penalty added to the objective function. This serves as a balance between the level of fit or ``measurement Phi"  $(\mathbf{\Phi_M})$ and the coherence with soft knowledge/previous conditions/prior knowledge/regularization $(\mathbf{\Phi_R})$. These terms can also be interpreted as the likelihood function and prior distribution in Bayes' theorem (see below) 
\item [PHIMLIM] A PEST input parameter the governs the strength with which regularization is applied to the objective function. A high value of PHIMLIM indicates a strong penalty for deviation from preferred parameter conditions while a low value of PHIMLIM indicates a weak penalty. The reason this ``dial" is listed as a function of PHIM (e.g. $\mathbf{\Phi_M}$) is because it can then be interpreted as a limit on how well we want to fit the observation data.
\item [FOSM] \emph{fill this in}
\item [Gaussian (multivariate)] The equation for Gaussian (Normal) distribution for a single variable ($x$) is 
\begin{equation}
f(x|\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}\frac{\left(x-\mu\right)^2}{\sigma^2}}
\end{equation}
where $\mu$ is the mean and $\sigma$ is the standard deviation
The equation for a multivariate Gaussian for a vector of $k$ variables ($\mathbf{x}$) is
\begin{equation}
f(\mathbf{x} | \mathbf{\mu},\mathbf{\Sigma})=\frac{1}{\sqrt{(2\pi)^k\left|\mathbf{\Sigma}\right|}}e^{-\frac{1}{2}\left( \left(\mathbf{x}-\mathbf{\mu} \right)^T  \mathbf{\Sigma}^{-1}\left(\mathbf{x}-\mathbf{\mu} \right)\right)}
\end{equation}
where $\mu$ is a $k$-length vector of mean values, $\mathbf{\Sigma}$ is the covariance matrix, and $\left|\mathbf{\Sigma}\right|$ is the determinant of the covariance matrix
\item [Weight] A value which a residual is divided by when constructing the sum of squared residuals. In principal, $w\approx\frac{1}{\sigma}$ where $\sigma$ is an approximation of the expected error between model output and collocated observation values. While the symbol $\sigma$ implies a standard deviation, it is important to note that measurement error only makes up a portion of this error. Other aspects such as structural error (e.g. inadequacy inherent in all models to perfectly simulate the natural world) also contribute to this expected level of error.
\item [Weight Covariance matrix (correlation matrix)] In practice, this is usually a $NOBS\times NOBS$ diagonal matrix with values of weights on the diagonal representing the inverse of the observation covariance. This implies a lack of correlation among the observations. A full covariance matrix would indicate correlation among the observations which, in reality, is present but, in practice, is rarely characterized. The weight matrix is often identified as $\mathbf{R}^{-1}$ or $\mathbf{\Sigma_\epsilon}^{-1}$
\item [Parameter Covariance matrix] The uncertainty of parameters can be expressed as a matrix as well. This is formed also as a diagonal matrix from the bounds around parameter values (assuming that the range between the bounds indicates $4\sigma$ (e.g. 95\% of a normal distribution). In \texttt{pyemu}, some functions accept a \texttt{sigma\_range} argument which can override the $4\sigma$ assumption. In many cases of our applications, parameters are spatially distributed (e.g. hydraulic conductivity fields) so a 
\item [Measurement noise/error]
\item [Structural (model) error]
\item [Monte Carlo Ensemble]
\item [Bayes' Theorem]
\item [Posterior (multivariate distribution)]
\item [Schur Complement]
\item [Prior (multivariate distribution)]
\item [Likelihood (multivariate distribution)] 

\end{description}

\end{document}