{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PEST++V3_cover.jpeg\" style=\"float: left\">\n",
    "\n",
    "<img src=\"flopylogo.png\" style=\"float: right\">\n",
    "\n",
    "<img src=\"AW&H2015.png\" style=\"float: center\">\n",
    "\n",
    "# SINGULAR VALUE DECOMPOSITION (SVD)\n",
    "\n",
    ">## \"A singularly valuable decomposition\" \n",
    ">--Dan Kalman \n",
    "\n",
    ">## \"Singular Value Decomposition. Love it, learn it.\"\n",
    ">--Michael Basial\n",
    "\n",
    ">## \"SVD? Magic, simply magic.\"\n",
    ">--John Doherty\n",
    "\n",
    "### As we've said, the key to representative environmental models is allowing high levels of flexibility through a highly parameterized approach.  But this makes our parameter estimation problem illposed and underdetermined, which means our solution is nonunique even if we overcome problems of increased instability and longer runtimes.  Here we use a \"regularized inversion\" approach to overcome these problems.  Regularization is anything that makes an intractable problem solvable; for example, using a small number of zones (not highly-parameterized) is a way to regularize an illposed problem.  Regularization as we use here can be grouped into two broad categories: 1) adding soft-knowledge to the problem (Tikhonov regularization) and 2) mathematically reducing the dimensionality of the model (subspace regularization via singular value decomposition (SVD)). In practice we typically use a combination (\"hybrid\") of these two approaches. \n",
    "\n",
    "### It is worth expounding on this difference in regularization approaches. In contrast to Tikhonov regularization, which adds information to the calibration process to achieve numerical stability, subspace methods achieve stability through subtracting parameters, and/or parameter combinations, from the calibration process (making a \"subspace\" of the full parameter space). Now the calibration process is no longer required to estimate either individual parameters or combinations of correlated parameters that are inestimable given the calibration dataset we have. What combinations are estimable are automatically determined through SVD. \n",
    "\n",
    "### The effort needed to take advantage of these regularization strategies is also appreciably different, where SVD is relatively easily brought to bear and becomes \"set it and forget it\". Moreover, when SVD is used the parameter estimation problem always becomes __unconditionally stable__! Neither of these is true in all cases when adding soft knowledge using Tikhonov regularization. \n",
    "\n",
    "### In summary, SVD benefits apply to all models so it is worth widely invoking when using PEST and PEST++.  SVD involves few parameter estimation inputs, default values work for a large range of problems, and it addresses instability for all problems. Can you catch that we can't overemphasize the importance of SVD to parameter estimation?  \"Magic\" indeed! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this and the next notebooks we'll get under the hood of SVD and see what it does.  A high-level understanding is not needed to take advantage of the power of SVD for your typical calibration parameter estimation problem (\"set it and forget it\").  BUT in addition to the glow of knowledge that they impart, these SVD concepts will cascade into understanding other tools such as Parameter Identifiability, calculation of uncertainty, and null-space Monte Carlo.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's another extra mathy expose on SVD: https://gist.github.com/frankcleary/a89da479d85c98f86e31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pyemu\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import cm \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import flopy as flopy\n",
    "import numpy as np\n",
    "noPIL=True\n",
    "try:\n",
    "    from PIL import Image\n",
    "except:\n",
    "    noPIL=True\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra is the foundation of much of our maths and modeling. At the basis of this is matrices, which are containing vector information like spatial array of properties, mappings from one set of properties to another, the variability of properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another example of a matrix is just a photograph. It turns out, much of the information contained in a matrix is redundant. If we think of the columns of a matrix as vectors, they are orthogonal but maybe aren't quite the right basis for the infromation. What if we could find another basis, where we rotate to a more suitable set of orthogonal basis vectors and maybe even stretch them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any matrix can be decomposed into 3 matrices\n",
    "## <center> $\\mathbf{M}=\\mathbf{U}\\mathbf{S}\\mathbf{V}^T$ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's evaluate how this works by exploring the information content in an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo = Image.open('clands.jpg')\n",
    "\n",
    "plt.imshow(photo, interpolation='nearest')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to grayscale\n",
    "\n",
    "### By converting to grayscale, what we are left with is a matrix of information where each pixel (e.g. a cell in rows/columns of the matrix) has a value between 0 and 255 indicating intensity. This is then just a matrix with information in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not noPIL:\n",
    "    photogray = np.array(photo.convert('L'))\n",
    "    np.savetxt('clands_gray.dat', photogray, fmt='%d')\n",
    "else:\n",
    "    photogray = np.loadtxt('clands_gray.dat', dtype=int)\n",
    "plt.imshow(photogray, interpolation='nearest', cmap='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can treat this like any matrix and perform SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, V = np.linalg.svd(photogray)\n",
    "print(photogray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigma)\n",
    "plt.grid()\n",
    "plt.title('{0} Singular values in descending order'.format(len(sigma)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sigma)\n",
    "plt.grid()\n",
    "plt.title('{0} Singular values in descending order'.format(len(sigma)));\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make a little function for using a subset of singular values to reconstitute the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_image(numsing=1):\n",
    "    reconimg = np.dot(np.dot(U[:,:numsing], np.diag(sigma[:numsing])),V[:numsing,:])\n",
    "    basis_vec = np.dot(np.dot(np.atleast_2d(U[:,numsing-1]).T, sigma[numsing-1]),np.atleast_2d(V[numsing-1,:]))\n",
    "    fig,ax = plt.subplots(ncols=2, figsize=(12,12))\n",
    "    ax[0].imshow(basis_vec, interpolation='nearest', cmap='gray')\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Basis Image')\n",
    "    ax[1].imshow(reconimg, interpolation='nearest', cmap='gray')\n",
    "    ax[1].axis('off')\n",
    "    ss = 's'\n",
    "    if numsing==1:\n",
    "        ss = ''\n",
    "    ax[1].set_title('Reconstruction using {0} singular value{1}'.format(numsing,ss))\n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(recon_image, numsing=widgets.widgets.IntSlider(value=1, min=1, max=300, step=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun times, but what does this have to do with modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load up a Jacobian Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join('..','master_glm','freyberg_pp.jcb')):\n",
    "    injac = pyemu.Jco.from_binary(os.path.join('..','master_glm','freyberg_pp.jcb'))\n",
    "else:\n",
    "    injac = pyemu.Jco.from_binary('freyberg_pp.jcb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injac.df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join('..','master_glm','freyberg_pp.pst')):\n",
    "    inpst = pyemu.Pst(os.path.join('..','master_glm','freyberg_pp.pst'))\n",
    "else:\n",
    "    inpst = pyemu.Pst('freyberg_pp.pst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = inpst.observation_data.weight.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.diag(Q)\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = injac.df().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(np.abs(X)), interpolation='nearest', cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(np.abs(X[:50,:50])), interpolation='nearest', cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can form up the normal equations matrix (including weights) and take a look at it\n",
    "\n",
    "This matrix is $\\mathbf{X}^T\\mathbf{Q}\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XtQX=X.T.dot(Q).dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, sigma, V = np.linalg.svd(XtQX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(sigma)),sigma)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(sigma)),sigma)\n",
    "plt.yscale('log')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(XtQX), interpolation='nearest', cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(V[:13,:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SV_bars(SV=1):\n",
    "    plt.figure(figsize=(13,4))\n",
    "    plt.bar(list(range(U.shape[0])),U[:,SV-1])\n",
    "    plt.grid()\n",
    "    #plt.yscale('log')\n",
    "    plt.xlim([0,75])\n",
    "    plt.ylim([-1,1])\n",
    "    plt.xticks(list(range(75)))\n",
    "    plt.title('Singular vector showing parameter contributions to singular vector #{0}'.format(SV))\n",
    "    plt.gca().set_xticklabels(inpst.parameter_data['parnme'], rotation=90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(SV_bars, SV=widgets.widgets.IntSlider(value=1, min=1, max=20, step=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great - finally how does this impact our calibration of a K-field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls freyberg_truth/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = flopy.modflow.Modflow.load('freyberg.truth.nam',model_ws='freyberg_truth',load_only=[\"upw\", \"bas6\"],check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pyemu.geostats.ExpVario(1.0,a=200,anisotropy=1.0,bearing=45)\n",
    "struct = pyemu.geostats.GeoStruct(variograms=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = pyemu.helpers.kl_setup(num_eig=800,sr=m.sr,struct=struct,prefixes='hk',basis_file=\"basis.jco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = pyemu.Matrix.from_binary(\"basis.jco\").to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = basis.index.map(lambda x: int(x[1:5]))\n",
    "j = basis.index.map(lambda x: int(x[-4:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's check out a continuous K field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.loadtxt(os.path.join(\"freyberg_truth\",\"hk.truth.ref\"))\n",
    "plt.figure(figsize=(4,8))\n",
    "mm = flopy.plot.PlotMapView(model=m, ax=plt.gca())\n",
    "mm.plot_array(arr)\n",
    "hk_true = arr.copy()\n",
    "mm.plot_ibound();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can reconstruct it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_arr = np.array(basis.values)\n",
    "flat_arr = np.atleast_2d(arr.flatten()).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_enchilada(eig):\n",
    "    fig,ax = plt.subplots(ncols=3, figsize=(14,8))\n",
    "    arr = np.zeros((m.nrow,m.ncol))\n",
    "    arr[i,j] = basis.iloc[:,eig]\n",
    "    mm = flopy.plot.PlotMapView(model=m, ax=ax[0])\n",
    "    mm.plot_array(arr)\n",
    "    mm.plot_ibound()\n",
    "    #mm = plt.imshow(arr)\n",
    "    ax[0].set_title('Plot of individual SV\\nnative scale')\n",
    "    basis_eig = basis_arr[:,:eig+1].transpose()\n",
    "    factors = np.dot(basis_eig,flat_arr).transpose()\n",
    "    factors = np.dot(factors,basis_eig).reshape(arr.shape)\n",
    "    mm2 = flopy.plot.PlotMapView(model=m, ax=ax[1])\n",
    "    mm2.plot_array(factors, vmin=0,vmax=np.max(hk_true))\n",
    "    mm2.plot_ibound()\n",
    "    ax[1].set_title('Reconstructed field\\nTrue Scale')\n",
    "    ax[2].set_title('True Field')    \n",
    "    mm3 = flopy.plot.PlotMapView(model=m, ax=ax[2])\n",
    "    c = mm3.plot_array(hk_true,vmin=0,vmax=np.max(hk_true))\n",
    "    mm3.plot_ibound()\n",
    "    plt.suptitle('Using {0} SVs'.format(eig+1))\n",
    "    plt.colorbar(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interact(plot_enchilada, eig=widgets.IntSlider(description=\"eig comp:\", \n",
    "                                           continuous_update=True, max=799));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
