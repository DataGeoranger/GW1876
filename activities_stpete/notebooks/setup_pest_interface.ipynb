{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the PEST(++) interface around the enhanced Freyberg model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will construct a complex model independent (non-intrusive) interface around an existing `MODFLOW-NWT` model using the `python/flopy/pyemu` stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import flopy\n",
    "import pyemu\n",
    "import prep_deps\n",
    "import redis\n",
    "import matplotlib as mpl\n",
    "plt.rcParams['font.size']=12\n",
    "%matplotlib inline\n",
    "pyemu.__path__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a base directory `b_d` from which we will read in a model already created `freyberg.nam`. This will form the basis of the remainder of the exercise (and those to follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_d = os.path.join(\"temp_history\")\n",
    "nam_file = \"freyberg.nam\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the existing Freyberg model. This version should run but is not yet connected with `PEST++`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that to load a model in a different folder, you supply the namefile without path and supply the path\n",
    "# to it in the model_ws variable\n",
    "m = flopy.modflow.Modflow.load(nam_file,model_ws=b_d,check=False,forgive=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some model attributes\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "ax = plt.subplot(111,aspect=\"equal\")\n",
    "mm = flopy.plot.ModelMap(model=m)\n",
    "mm.plot_grid()\n",
    "mm.plot_ibound()\n",
    "mm.plot_bc('SFR')\n",
    "ax = mm.ax\n",
    "#m.wel.stress_period_data.plot(ax=ax,mflay=2)\n",
    "\n",
    "# plot obs locations\n",
    "obs = pd.read_csv(os.path.join(\"..\",\"base_model_files\",\"obs_loc.csv\"))\n",
    "                  \n",
    "obs_x = [m.sr.xcentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "obs_y = [m.sr.ycentergrid[r-1,c-1] for r,c in obs.loc[:,[\"row\",\"col\"]].values]\n",
    "ax.scatter(obs_x,obs_y,marker='.',label=\"water-level obs\",s=80)\n",
    "\n",
    "#plot names on the pumping well locations\n",
    "wel_data = m.wel.stress_period_data[0]\n",
    "wel_x = m.sr.xcentergrid[wel_data[\"i\"],wel_data[\"j\"]]\n",
    "wel_y = m.sr.ycentergrid[wel_data[\"i\"],wel_data[\"j\"]]\n",
    "for i,(x,y) in enumerate(zip(wel_x,wel_y)):\n",
    "    ax.scatter([x],[y],color=\"red\",marker=\"s\",s=50)\n",
    "    #ax.text(x,y,\"{0}\".format(i+1),ha=\"center\",va=\"center\")\n",
    "\n",
    "ax.set_ylabel(\"y(m)\")\n",
    "ax.set_xlabel(\"x(m)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we can do a couple `flopy` things to move where the new model will be written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the executable name for the model\n",
    "m.exe_name = \"mfnwt\"\n",
    "\n",
    "# now let's run this in a new folder called temp so we don't overwrite the original data\n",
    "m.change_model_ws(\"temp\",reset_external=True)\n",
    "\n",
    "# this writes all the MODFLOW files in the new location \n",
    "m.write_input()\n",
    "\n",
    "# the following helps get the dependecies (both python and executables) in the right place\n",
    "prep_deps.prep_template(t_d=\"temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we can run the model once using a `pyemu` helper\n",
    "This helper is particularly useful if you run on more than one platform (e.g. Mac and Windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"{0} {1}\".format(m.exe_name,m.name+\".nam\"),cwd=m.model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read in the heads and plot them up along with the budget components\n",
    "Note that there is a historic period and a scenario with future conditions that differ. \n",
    "\n",
    "_For the future scenario, a serious drought, recharge is lower and pumping/abstraction is increased to make up for the presumed deficite in water for agriculture._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "hds = flopy.utils.HeadFile(os.path.join(m.model_ws,m.name+\".hds\"),model=m)\n",
    "hds.plot(mflay=0)\n",
    "lst = flopy.utils.MfListBudget(os.path.join(m.model_ws,m.name+\".list\"))\n",
    "df = lst.get_dataframes(diff=True)[0]\n",
    "plt.figure()\n",
    "ax = df.plot(kind=\"bar\",figsize=(30,30), grid=True,subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the effect of the \"scenario\" in the second stress period with less recharge and more abstraction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot depth to water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw = m.dis.top.array - hds.get_data()[0,:,:]\n",
    "dtw = np.ma.masked_where(m.bas6.ibound[0].array==0,dtw)\n",
    "c = plt.imshow(dtw)\n",
    "plt.title('Depth to Water')\n",
    "plt.colorbar(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the river and well locations expressed in the depth to water pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data structures related to what we want to parameterize and what we want to observe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first the parameterization of model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = []\n",
    "# here we specify which packages we wish to parameterize, \n",
    "# starting with those that do not change over time\n",
    "paks = [\"upw.hk\",\"upw.vka\",\"upw.ss\",\"upw.sy\",\"extra.prsity\"]  #\"extra\" because not a modflow parameter\n",
    "for k in range(m.nlay):\n",
    "    props.extend([[p,k] for p in paks])\n",
    "const_props = props.copy()\n",
    "props.append([\"rch.rech\",None])\n",
    "for kper in range(m.nper):\n",
    "    const_props.append([\"rch.rech\",kper])\n",
    "props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we want to handle list-type parameters in two ways\n",
    "for `spatial_list_props` this will apply a multiplier distributed spatially that applied in all stress periods throughout the model\n",
    "\n",
    "for `temporal_list_props` this will apply a multiplier for each stress period applied to all the spatial locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spatial_list_props = [[\"wel.flux\",2],[\"drn.cond\",0]]  # spatially by each list entry, across all stress periods\n",
    "#temporal_list_props = [[\"wel.flux\",kper] for kper in range(m.nper)]  # spatially uniform for each stress period\n",
    "spatial_list_props = []\n",
    "temporal_list_props = []\n",
    "spatial_list_props, temporal_list_props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next we want to set up the extraction of model outputs for which we have observations. First, we will setup a post-processor that will read the heads for all active cells in both stress periods - why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dry_kper = int(m.nper * 0.75)\n",
    "hds_kperk = [[kper,k] for k in range(m.nlay) for kper in [0,dry_kper,m.nper-1]]\n",
    "#hds_kperk.extend([[1,k] for k in range(m.nlay)])\n",
    "\n",
    "hds_kperk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### then we setup monitoring of the SFR ASCII outputs.  \n",
    "we will accumulate the first 20 reaches and last 20 reaches (corresponding to the top and bottom half of the model, respectively) together to form forecasts of sw-gw exchange in the headwaters (`hw`) and tailwaters (`tw`).  Then we will also add each reach individually for monitoring as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr_obs_dict = {}\n",
    "sfr_obs_dict[\"hw\"] = np.arange(1,int(m.nrow/2))\n",
    "sfr_obs_dict[\"tw\"] = np.arange(int(m.nrow/2),m.nrow)\n",
    "sfr_obs_dict[\"gage_1\"] = [39]\n",
    "#for i in range(m.nrow):\n",
    "#    sfr_obs_dict[i] = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here we go...\n",
    "\n",
    "This `pyemu` class has grown into a monster...it does (among other things):\n",
    "- sets up combinations of multiplier parameters for array inputs, including uniform, zones, pilot points, grids, and KL expansion types\n",
    "- sets up combinations of multiplier parameters for list inputs\n",
    "- handles several of the shitty modflow exceptions to the array and list style inputs\n",
    "- sets up large numbers of observations based on arrays or time series\n",
    "- writes .tpl, .ins, .pst, etc\n",
    "- writes a python forward run script\n",
    "- writes a prior parameter covaraince matrix using geostatistical correlations\n",
    "- draws from the prior parameter covariance matrix to generate a prior parameter ensemble\n",
    "\n",
    "WAT?!\n",
    "\n",
    "This will be slow because the pure python kriging...but, hey, its free!\n",
    "\n",
    "For our purposes, we will setup combinations of constant (by layer), pilot points and grid-scale parameters for each of the array-based properties we defined earlier.  This lets us explore options for parameterization and also start to understand how information flows in the history matching problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_helper = pyemu.helpers.PstFromFlopyModel(nam_file,new_model_ws=\"template_history\",org_model_ws=\"temp\",\n",
    "                                             const_props=const_props,spatial_list_props=spatial_list_props,\n",
    "                                             temporal_list_props=temporal_list_props,remove_existing=True,\n",
    "                                             pp_props=props,grid_props=props,sfr_pars=True,hds_kperk=hds_kperk,\n",
    "                                             sfr_obs=sfr_obs_dict,build_prior=False,model_exe_name=\"mfnwt\",\n",
    "                                             pp_space=4)\n",
    "prep_deps.prep_template(t_d=pst_helper.new_model_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `pst_helper` instance contains the `pyemu.Pst` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so, pull out the `pyemu.Pst` instance which \n",
    "#contains all the input that ultimately goes in the PEST control %%file\n",
    "pst = pst_helper.pst\n",
    "pst.npar,pst.nobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oh snap!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyemu` uses `pandas` data frame format for the parameter and observation data sections. This offers plenty of querying and bulk editing options.\n",
    "\n",
    "Let's stop for a moment to get a better feel for what just happened! Let's dig in.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out hydraulic conductivity parameters\n",
    "pst.parameter_data.loc[pst.parameter_data.parnme.apply(lambda x: \"hk\" in x),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about observations? in particular, the sfr flow-out observations?\n",
    "pst.observation_data.loc[pst.observation_data.obgnme.apply(lambda x: \"flout\" in x),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "flout_obs = obs.loc[obs.obgnme.apply(lambda x: \"flout\" in x),\"obsnme\"]\n",
    "obs.loc[flout_obs,\"obgnme\"] = flout_obs.apply(lambda x: \"_\".join(x.split('_')[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add time-series based observations\n",
    "\n",
    "We need to track specific head locations across all times for transient observations.  To setup the tracking, we need to know where (lay-row-col) the observations are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_locs = pd.read_csv(os.path.join(\"..\",\"base_model_files\",\"obs_loc.csv\"))\n",
    "#build obs names that correspond to the obsnme values in the control file\n",
    "obs_locs.loc[:,\"site\"] = obs_locs.apply(lambda x: \"trgw_{0:03d}_{1:03d}\".format(x.row-1,x.col-1),axis=1)\n",
    "kij_dict = {site:(0,r-1,c-1) for site,r,c in zip(obs_locs.site,obs_locs.row,obs_locs.col)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "binary_file = os.path.join(pst_helper.m.model_ws,nam_file.replace(\".nam\",\".hds\"))\n",
    "frun_line,tr_hds_df = pyemu.gw_utils.setup_hds_timeseries(binary_file,kij_dict=kij_dict,include_path=True,model=pst_helper.m)\n",
    "pst_helper.frun_post_lines.append(frun_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_hds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in os.listdir(pst_helper.m.model_ws) if f.endswith(\".ins\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pst_helper.pst.add_observations(os.path.join(pst_helper.m.model_ws,\n",
    "                nam_file.replace(\".nam\",\".hds_timeseries.processed.ins\")),pst_path=\".\")\n",
    "obs = pst_helper.pst.observation_data\n",
    "obs.loc[df.index,\"obgnme\"] = df.index.map(lambda x: \"_\".join(x.split(\"_\")[:-1]))\n",
    "obs.loc[df.index,\"weight\"] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add modpath input files, instruction files and calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First copy over all the MODPATH-related files from the base directory identified in the `b_d` variable.   We will track a single particle for forecast purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_files = [f for f in os.listdir(b_d) if \"mp\" in f or \"location\" in f]\n",
    "[shutil.copy2(os.path.join(b_d,f),os.path.join(pst_helper.new_model_ws,f)) for f in mp_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `frun_post_lines` property adds statements at the end of the `forward_run.py` script. In this case, it runs MODPATH using `mp6`.  We will also identify any additional temporary files that the forward run script will attempt to remove at the start of a run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pst_helper.frun_post_lines.append(\"os.system('mp6 freyberg.mpsim >mp6.stdout')\")\n",
    "pst_helper.frun_post_lines.append(\"pyemu.os_utils.run('mp6 freyberg.mpsim >mp6.stdout')\")\n",
    "pst_helper.tmp_files.append(\"freyberg.mpenpt\")  # placed at top of `forward_run.py`\n",
    "pst_helper.write_forward_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and add instruction files and related observations for MODPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = \"freyberg.mpenpt\"\n",
    "ins_file = out_file + \".ins\"\n",
    "with open(os.path.join(pst_helper.new_model_ws,ins_file),'w') as f:\n",
    "    f.write(\"pif ~\\n\")\n",
    "    f.write(\"l7 w w w !part_status! w w !part_time!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pst_helper.pst.add_observations(os.path.join(pst_helper.new_model_ws,ins_file),\n",
    "                                     os.path.join(pst_helper.new_model_ws,out_file),\n",
    "                                     pst_path=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to copy the original prsity arrays to the `arr_org` dir for use in the multiplier parameterization scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(m.nlay):\n",
    "    np.savetxt(os.path.join(pst_helper.new_model_ws,\"arr_org\",\"prsity_layer_{0}.ref\".format(k+1)),\n",
    "               np.zeros((m.nrow,m.ncol))+0.001,fmt=\"%15.6E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final bits and bobs\n",
    "We need to set some realistic parameter bounds and account for expected (but stochastic) scenario conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst.parameter_data  # we inspected this guy earlier\n",
    "# properties\n",
    "tag_dict = {\"hk\":[0.1,10.0],\"vka\":[0.1,10],\"strt\":[0.95,1.05],\"pr\":[0.8,1.2]}\n",
    "for t,[l,u] in tag_dict.items():\n",
    "    t_pars = par.loc[par.parnme.apply(lambda x: t in x ),\"parnme\"]\n",
    "    par.loc[t_pars,\"parubnd\"] = u\n",
    "    par.loc[t_pars,\"parlbnd\"] = l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given the combinations of multipliers, we need to set a hard upper bound on sy since it has a physical upper limit (note: seperate to bounds handled explicitly by pest).  This is how you set bounds on the actual model input arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_csv = os.path.join(pst_helper.new_model_ws,\"arr_pars.csv\")\n",
    "df = pd.read_csv(arr_csv,index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy_pr = df.model_file.apply(lambda x: \"sy\" in x or \"pr\" in x)\n",
    "df.loc[:,\"upper_bound\"] = np.NaN\n",
    "df.loc[sy_pr,\"upper_bound\"] = 0.4\n",
    "#rch = df.model_file.apply(lambda x: \"rech\" in x)\n",
    "#df.loc[rch,\"upper_bound\"] = 1000.0\n",
    "df.to_csv(arr_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table can also be written to a .tex file (report-ready!)\n",
    "pst.write_par_summary_table(filename=\"none\").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write_obs_summary_table(filename=\"none\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the process once (`noptmax=0`) to make sure its all plumbed up.  Pro-tip: you can use any of the `pestpp-###` binaries/executables to run `noptmax=0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.control_data.noptmax = 0\n",
    "pst.write(os.path.join(pst_helper.new_model_ws,\"freyberg.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-ies freyberg.pst\",cwd=pst_helper.new_model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(pst_helper.m.model_ws,\"freyberg.pst\"))\n",
    "#assert pst.phi < 1.0e-10,pst.phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take it up a notch. We need to generate the prior parameter covariance matrix and stochastic realizations.  We will use the geostatistical covariance information in the `pst_helper` instance for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pst_helper.pst.npar < 35000:\n",
    "    cov = pst_helper.build_prior(fmt=\"coo\",filename=os.path.join(pst_helper.new_model_ws,\"prior_cov.jcb\"))\n",
    "    cov = np.ma.masked_where(cov.x==0,cov.x)\n",
    "    try:\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax = plt.subplot(111)\n",
    "        ax.imshow(cov)\n",
    "        plt.show()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we can make a draw from the prior parameter covariance matrix to form a prior parameter ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pst_helper.draw(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that parameters are treated in parameter group (`pargp`) blocks for this ensemble generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always a good idea to inspect the parameter ensemble for reasonableness! Can do via slicing and dicing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.iloc[-10:-5,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parameters by groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = pst_helper.pst.parameter_data\n",
    "pyemu.plot_utils.ensemble_helper(pe,plot_cols=par.groupby(\"pargp\").groups,bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts? Do these look reasonable? We see log-normal distributions for log-transformed parameters, e.g., hk... looking good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to enforce parameter bounds and save this ensemble for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe.enforce()  # always a good idea!\n",
    "pe.to_binary(os.path.join(pst_helper.new_model_ws,\"prior.jcb\"))\n",
    "pst_helper.pst.write(os.path.join(pst_helper.m.model_ws,nam_file.replace(\".nam\",\".pst\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set forecast names - just a few for FOSM later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst_helper.pst.observation_data\n",
    "dts = pd.to_datetime(pst_helper.m.start_datetime) + pd.to_timedelta(np.cumsum(pst_helper.m.dis.perlen.array),unit='d')\n",
    "dts_str = list(dts.map(lambda x: x.strftime(\"%Y%m%d\")).values)\n",
    "dry_kper = int(pst_helper.m.nper * 0.75)\n",
    "dry_dt = dts_str[dry_kper]\n",
    "print(dry_dt)\n",
    "swgw_forecasts = obs.loc[obs.obsnme.apply(lambda x: \"fa\" in x and (\"hw\" in x or \"tw\" in x) and dry_dt in x),\"obsnme\"].tolist()\n",
    "#print(swgw_forecasts)\n",
    "hds_fore_name = \"hds_00_{0:03d}_{1:03d}_{2:03d}\".format(int(pst_helper.m.nrow/3),int(pst_helper.m.ncol/10)\n",
    "                                                       ,dry_kper)\n",
    "print(hds_fore_name)\n",
    "hds_forecasts = obs.loc[obs.obsnme.apply(lambda x: hds_fore_name in x),\"obsnme\"].tolist()\n",
    "forecasts = swgw_forecasts\n",
    "forecasts.extend(hds_forecasts)\n",
    "forecasts.append(\"part_time\")\n",
    "forecasts.append(\"part_status\")\n",
    "pst_helper.pst.pestpp_options[\"forecasts\"] = forecasts\n",
    "forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst_helper.pst.write(os.path.join(pst_helper.m.model_ws,nam_file.replace(\".nam\",\".pst\")))\n",
    "pyemu.os_utils.run(\"pestpp-ies {0}\".format(nam_file.replace(\".nam\",\".pst\")),cwd=pst_helper.m.model_ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = flopy.utils.MfListBudget(os.path.join(pst_helper.m.model_ws,nam_file.replace(\".nam\",\".list\")))\n",
    "df = lst.get_dataframes(diff=True,start_datetime=pst_helper.m.start_datetime)[0]\n",
    "df.plot(kind=\"bar\",figsize=(30,30), grid=True,subplots=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demystifying the multiplier parameter process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_plot_hk(real):\n",
    "    # replace the par values in the control file\n",
    "    pst.parameter_data.loc[:,\"parval1\"] = pe.loc[real,pst.par_names]\n",
    "    # save the updated control file\n",
    "    pst.write(os.path.join(pst_helper.new_model_ws,\"test.pst\"))\n",
    "    # run a single model run to generate the multipliers and inputs\n",
    "    pyemu.os_utils.run(\"pestpp-ies.exe test.pst\",cwd=pst_helper.new_model_ws)\n",
    "\n",
    "    # load the arrays\n",
    "    base_arr = np.log10(np.loadtxt(os.path.join(pst_helper.new_model_ws,\"arr_org\",\"hk_Layer_1.ref\")))\n",
    "    pp_arr = np.log10(np.loadtxt(os.path.join(pst_helper.new_model_ws,\"arr_mlt\",\"hk0.dat_pp\")))\n",
    "    gr_arr = np.log10(np.loadtxt(os.path.join(pst_helper.new_model_ws,\"arr_mlt\",\"hk3.dat_gr\")))\n",
    "    cn_arr = np.log10(np.loadtxt(os.path.join(pst_helper.new_model_ws,\"arr_mlt\",\"hk6.dat_cn\")))\n",
    "    in_arr = np.log10(np.loadtxt(os.path.join(pst_helper.new_model_ws,\"hk_Layer_1.ref\")))\n",
    "    arrs = [base_arr,cn_arr,pp_arr,gr_arr,in_arr]\n",
    "    \n",
    "    labels = [\"log10 base\",\"log10 constant\",\"log10 pilot points\",\"log10 grid\",\"log10 resulting input\"]\n",
    "    # mask with ibound\n",
    "    ib = m.bas6.ibound[0].array\n",
    "    for i,arr in enumerate(arrs):\n",
    "        arr[ib==0] = np.NaN\n",
    "    \n",
    "    fig,axes = plt.subplots(1,5,figsize=(20,5))\n",
    "    \n",
    "    # work out the multiplier min and max\n",
    "    vmin1 = min([np.nanmin(a) for a in arrs[1:-1]])\n",
    "    vmax1 = max([np.nanmax(a) for a in arrs[1:-1]])\n",
    "    \n",
    "    # plot each array\n",
    "    for i,(ax,arr,label) in enumerate(zip(axes,arrs,labels)):\n",
    "        if i not in [0,len(arrs)-1]:  \n",
    "            cb = ax.imshow(arr,vmin=vmin1,vmax=vmax1)\n",
    "        else:\n",
    "            cb = ax.imshow(arr)\n",
    "        ax.set_title(label)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        plt.colorbar(cb,ax=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_and_plot_hk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_and_plot_hk(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
