{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run and process the prior monte carlo and pick a \"truth\" realization\n",
    "\n",
    "A great advantage of exploring a synthetic model is that we can enforce a \"truth\" and then evaluate how our various attempts to estimate it perform. One way to do this is to run a monte carlo ensemble of multiple parameter realizations and then choose one of them to represent the \"truth\". That will be accomplished in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.rcParams['font.size']=12\n",
    "import flopy\n",
    "import pyemu\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPER IMPORTANT: SET HOW MANY PARALLEL WORKERS TO USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the `t_d` or \"template directory\" variable to point at the template folder and read in the PEST control file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_d = \"template\"\n",
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.npar_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.pestpp_options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the previously generated parameter ensemble and inspect (again!)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pyemu.ParameterEnsemble.from_binary(pst=pst,filename=os.path.join(t_d,\"prior.jcb\"))\n",
    "#pe.loc[:,should_fix] = 1.0\n",
    "pe.to_csv(os.path.join(t_d,\"sweep_in.csv\"))\n",
    "pe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the prior ensemble in parallel locally\n",
    "This takes advantage of the program `pestpp-swp` which runs a parameter sweep through a set of parameters. By default, `pestpp-swp` reads in the ensemble from a file called `sweep_in.csv` which in this case we made just above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_d = \"master_truth_sweep\"\n",
    "pyemu.os_utils.start_workers(t_d,\"pestpp-swp\",\"freyberg.pst\",num_workers=num_workers,worker_root=\".\",master_dir=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the output ensemble and plot a few things\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = pd.read_csv(os.path.join(m_d,\"sweep_out.csv\"),index_col=0)\n",
    "print('number of realization in the ensemble before dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop any failed runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = obs_df.loc[obs_df.failed_flag==0,:]\n",
    "print('number of realization in the ensemble **after** dropping: ' + str(obs_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.iloc[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting the \"truth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just need to replace the observed values (`obsval`) in the control file with the outputs for one of the realizations in `obs_df` that we consider to be the ``truth``.  In this way, we now have the nonzero values for history matching, but also the ``truth`` values for comparing how we are doing with other unobserved quantities.  I'm going to pick a realization that yields an \"average\" variability of the observed gw levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.forecast_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = pst.forecast_names[0]\n",
    "print(forecast)\n",
    "sorted_vals = obs_df.loc[:,forecast].sort_values()\n",
    "sorted_vals[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.loc[:,forecast].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = sorted_vals.index[90]\n",
    "print(obs_df.loc[idx,forecast])\n",
    "idx  # candidate truth realization index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the outputs corresponding to available observations and forecasts for this realization look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df.loc[idx,pst.nnz_obs_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how our selected truth does with the sw/gw forecasts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights!!!\n",
    "Assign some initial weights. Now, it is custom to add noise to the observed values...we will use the classic Gaussian noise...zero mean and standard deviation of 1 over the weight (which we will now specify).  We will speak more about noise and its sources shortly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(t_d,\"freyberg.pst\"))\n",
    "obs = pst.observation_data\n",
    "obs.loc[:,\"weight\"] = 0.0\n",
    "obs.loc[:,\"obsval\"] = obs_df.loc[idx,pst.obs_names]\n",
    "obs.loc[obs.obsnme.apply(lambda x: \"trgw\" in x),\"weight\"] = 5.0  # this corresponds to an (expected) noise standard deviation of 20 cm...\n",
    "obs.loc[obs.obsnme.apply(lambda x: \"fo_gage_1\" in x),\"weight\"] = 0.01  # corresponding to an (expected) noise standard deviation of 100 m^3/d...\n",
    "obs.loc[pst.nnz_obs_names,\"weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we just get a sample from a random normal distribution with mean=0 and std=1.\n",
    "The argument indicates how many samples we want - and we choose `pst.nnz_obs` which is the \n",
    "the number of nonzero-weighted observations in the PST file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "snd = np.random.randn(pst.nnz_obs)\n",
    "noise = snd * 1./obs.loc[pst.nnz_obs_names,\"weight\"]\n",
    "pst.observation_data.loc[noise.index,\"obsval\"] += noise\n",
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=0)\n",
    "nz_obs = obs.loc[pst.nnz_obs_names,:].copy()\n",
    "trgw_obs = nz_obs.loc[nz_obs.obsnme.apply(lambda x: \"trgw\" in x),:].copy()\n",
    "trgw_snd = np.random.randn(trgw_obs.shape[0])\n",
    "trgw_noise = trgw_snd * 1./trgw_obs.weight\n",
    "pst.observation_data.loc[trgw_obs.obsnme,\"obsval\"] += trgw_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "\n",
    "fo_obs = nz_obs.loc[nz_obs.obsnme.apply(lambda x: \"fo_\" in x),:].copy()\n",
    "fo_snd = np.random.randn(fo_obs.shape[0])\n",
    "fo_noise = fo_snd * (fo_obs.obsval * 0.1)\n",
    "pst.observation_data.loc[fo_obs.obsnme,\"obsval\"] += fo_noise\n",
    "fo_noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Just for fun, lets have some \"model error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pst.observation_data.loc[\"hds_00_009_001_000\",\"obsval\"] = obs_df.loc[:,\"hds_00_009_001_000\"].min() - 1.0\n",
    "#pst.observation_data.loc[\"hds_00_015_016_000\",\"obsval\"] = obs_df.loc[:,\"hds_00_015_016_000\"].min() - 1.0\n",
    "obs = pst.observation_data\n",
    "obs.loc[obs.obsnme.apply(lambda x: \"trgw_009_001\" in x),\"obsval\"] -= 1.0\n",
    "obs.loc[obs.obsnme.apply(lambda x: \"trgw_015_016\" in x),\"obsval\"] -= 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we write this out to a new file and run `pestpp-ies` to see how the objective function looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst.write(os.path.join(m_d,\"freyberg.pst\"))\n",
    "pyemu.os_utils.run(\"pestpp-ies freyberg.pst\",cwd=m_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can read in the results and make some figures showing residuals and the balance of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pst = pyemu.Pst(os.path.join(m_d,\"freyberg.pst\"))\n",
    "print(pst.phi)\n",
    "print(pst.nnz_obs_groups)\n",
    "plt.figure()\n",
    "pst.plot(kind='phi_pie');\n",
    "print('Here are the non-zero weighted observation contributions to phi')\n",
    "\n",
    "figs = pst.plot(kind=\"1to1\");\n",
    "pst.res.loc[pst.nnz_obs_names,:]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pst.observation_data.loc[pst.nnz_obs_names,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run the \"truth\" model once and inspect..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_df = pd.read_csv(os.path.join(m_d,\"sweep_in.csv\"),index_col=0)\n",
    "pst.parameter_data.loc[:,\"parval1\"] = par_df.loc[idx,pst.par_names]\n",
    "pst.write(os.path.join(m_d,\"test.pst\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will run this with `noptmax=0` to perform a single run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyemu.os_utils.run(\"pestpp-ies.exe test.pst\",cwd=m_d)\n",
    "pst = pyemu.Pst(os.path.join(m_d,\"test.pst\"))\n",
    "print(pst.phi)\n",
    "pst.res.loc[pst.nnz_obs_names,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "pst.plot(kind='phi_pie');\n",
    "print('Here are the non-zero weighted observation contributions to phi')\n",
    "\n",
    "figs = pst.plot(kind=\"1to1\");\n",
    "pst.res.loc[pst.nnz_obs_names,:]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual should be exactly the noise values from above. Lets load the model (that was just run using the true pars) and check some things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = flopy.modflow.Modflow.load(\"freyberg.nam\",model_ws=m_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = m.rch.rech[0].array\n",
    "#a = m.rch.rech[0].array\n",
    "a = np.ma.masked_where(m.bas6.ibound[0].array==0,a)\n",
    "print(a.min(),a.max())\n",
    "c = plt.imshow(a)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = flopy.utils.MfListBudget(os.path.join(m_d,\"freyberg.list\"))\n",
    "df = lst.get_dataframes(diff=True,start_datetime=m.start_datetime)[0]\n",
    "ax = df.plot(kind=\"bar\",figsize=(30,30), grid=True,subplots=True)\n",
    "#a = ax.set_xticklabels([\"historic\",\"scenario\"],rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### see how our existing observation ensemble compares to the truth\n",
    "\n",
    "forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pst.observation_data\n",
    "fnames = pst.pestpp_options[\"forecasts\"].split(\",\")\n",
    "plt.figure()\n",
    "for forecast in fnames:\n",
    "    ax = plt.subplot(111)\n",
    "    obs_df.loc[:,forecast].hist(ax=ax,color=\"0.5\",alpha=0.5)\n",
    "    ax.plot([obs.loc[forecast,\"obsval\"],obs.loc[forecast,\"obsval\"]],ax.get_ylim(),\"r\")\n",
    "    ax.set_title(forecast)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nz_obs = pst.observation_data.loc[pst.nnz_obs_names,:].copy()\n",
    "nz_obs.loc[:,\"datetime\"] = pd.to_datetime(nz_obs.obsnme.apply(lambda x: x.split(\"_\")[-1]))\n",
    "for nz_group in pst.nnz_obs_groups:\n",
    "    nz_obs_group = nz_obs.loc[nz_obs.obgnme==nz_group,:]\n",
    "    fig,ax = plt.subplots(1,1,figsize=(10,2))\n",
    "    ax.plot(nz_obs_group.datetime,nz_obs_group.obsval,\"r-\")\n",
    "    ax.plot(nz_obs_group.datetime,pst.res.loc[nz_obs_group.obsnme,\"modelled\"],\"b-\")\n",
    "    ax.set_title(nz_group)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for oname in pst.nnz_obs_names:\n",
    "#     ax = plt.subplot(111)\n",
    "#     obs_df.loc[:,oname].hist(ax=ax,color=\"0.5\",alpha=0.5)\n",
    "#     ax.plot([obs.loc[oname,\"obsval\"],obs.loc[oname,\"obsval\"]],ax.get_ylim(),\"r\")\n",
    "#     ax.set_title(oname)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the \"true\" obsvals to the obs loc info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_d = os.path.join(\"..\",\"..\",\"data\",\"freyberg_nwt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_obs = obs.loc[obs.obsnme.apply(lambda x: x in pst.nnz_obs_names or x in fnames),:]\n",
    "out_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_obs.to_csv(os.path.join(b_d,\"obs_data.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
